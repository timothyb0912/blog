<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="twitter:card" content="summary_large_image" /><!-- Begin Jekyll SEO tag v2.6.1 -->
<title>Prior Visualization | Timothy Brathwaite</title>
<meta name="generator" content="Jekyll v4.1.1" />
<meta property="og:title" content="Prior Visualization" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Primer on visualizing prior distributions." />
<meta property="og:description" content="Primer on visualizing prior distributions." />
<link rel="canonical" href="https://timothyb0912.github.io/blog/simulation/2020/08/31/Prior-Visualization.html" />
<meta property="og:url" content="https://timothyb0912.github.io/blog/simulation/2020/08/31/Prior-Visualization.html" />
<meta property="og:site_name" content="Timothy Brathwaite" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2020-08-31T00:00:00-05:00" />
<script type="application/ld+json">
{"mainEntityOfPage":{"@type":"WebPage","@id":"https://timothyb0912.github.io/blog/simulation/2020/08/31/Prior-Visualization.html"},"url":"https://timothyb0912.github.io/blog/simulation/2020/08/31/Prior-Visualization.html","@type":"BlogPosting","headline":"Prior Visualization","dateModified":"2020-08-31T00:00:00-05:00","datePublished":"2020-08-31T00:00:00-05:00","description":"Primer on visualizing prior distributions.","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/blog/assets/css/style.css"><link type="application/atom+xml" rel="alternate" href="https://timothyb0912.github.io/blog/feed.xml" title="Timothy Brathwaite" /><link rel="shortcut icon" type="image/x-icon" href="/blog/images/favicon.ico"><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/Primer/15.2.0/primer.css" integrity="sha512-xTz2ys4coGAOz8vuV1NcQBkgVmKhsSEtjbqyMJbBHRplFuvKIUo6xhLHpAyPt9mfR6twHJgn9OgVLuqOvjeBhg==" crossorigin="anonymous" />
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.14.0/css/all.min.css" integrity="sha512-1PKOgIY59xJ8Co8+NE6FZ+LOAZKjy+KY8iq0G4B3CyeY6wYHN3yt9PW0XpSriVlkMXe40PTKnXrLnZ9+fkDaog==" crossorigin="anonymous" />
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.12.0/katex.min.css" integrity="sha512-h7nl+xz8wgDlNM4NqKEM4F1NkIRS17M9+uJwIGwuo8vGqIl4BhuCKdxjWEINm+xyrUjNCnK5dCrhM0sj+wTIXw==" crossorigin="anonymous" />
    <script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.12.0/katex.min.js" integrity="sha512-/CMIhXiDA3m2c9kzRyd97MTb3MC6OVnx4TElQ7fkkoRghwDf6gi41gaT1PwF270W6+J60uTmwgeRpNpJdRV6sg==" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.12.0/contrib/auto-render.min.js" integrity="sha512-Do7uJAaHZm5OLrIv/yN4w0iG1dbu01kzdMNnFfu/mAqgUk6Nniv2JYHcwH+cNwjqgLcqcuBBk+JRvprLVI8azg==" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js" integrity="sha512-0doc9hKxR3PYwso42RD1p5ySZpzzuDiOwMrdCEh2WdJZCjcmFKc/wEnL+z8fBQrnHoiNWbo+3fiGkOYXBdQp4A==" crossorigin="anonymous"></script>
    <script>
    document.addEventListener("DOMContentLoaded", function() {
        renderMathInElement( document.body, {
        delimiters: [
            {left: "$$", right: "$$", display: true},
            {left: "[%", right: "%]", display: true},
            {left: "$", right: "$", display: false}
        ]}
        );
    });
    </script>


<script>
function wrap_img(fn) {
    if (document.attachEvent ? document.readyState === "complete" : document.readyState !== "loading") {
        var elements = document.querySelectorAll(".post img");
        Array.prototype.forEach.call(elements, function(el, i) {
            if (el.getAttribute("title") && (el.className != "emoji")) {
                const caption = document.createElement('figcaption');
                var node = document.createTextNode(el.getAttribute("title"));
                caption.appendChild(node);
                const wrapper = document.createElement('figure');
                wrapper.className = 'image';
                el.parentNode.insertBefore(wrapper, el);
                el.parentNode.removeChild(el);
                wrapper.appendChild(el);
                wrapper.appendChild(caption);
            }
        });
    } else { document.addEventListener('DOMContentLoaded', fn); }
}
window.onload = wrap_img;
</script>

<script>
    document.addEventListener("DOMContentLoaded", function(){
    // add link icon to anchor tags
    var elem = document.querySelectorAll(".anchor-link")
    elem.forEach(e => (e.innerHTML = '<i class="fas fa-link fa-xs"></i>'));
    });
</script>
</head>
<body><header class="site-header">

  <div class="wrapper"><a class="site-title" rel="author" href="/blog/">Timothy Brathwaite</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/blog/about/">About Me</a><a class="page-link" href="/blog/search/">Search</a><a class="page-link" href="/blog/categories/">Tags</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">Prior Visualization</h1><p class="page-description">Primer on visualizing prior distributions.</p><p class="post-meta post-meta-title"><time class="dt-published" datetime="2020-08-31T00:00:00-05:00" itemprop="datePublished">
        Aug 31, 2020
      </time>
       • <span class="read-time" title="Estimated read time">
    
    
      6 min read
    
</span></p>

    
      <p class="category-tags"><i class="fas fa-tags category-tags-icon"></i></i> 
      
        <a class="category-tags-link" href="/blog/categories/#simulation">simulation</a>
        
      
      </p>
    

    </header>

  <div class="post-content e-content" itemprop="articleBody">
    <ul class="section-nav">
<li class="toc-entry toc-h1"><a href="#visualizing-prior-distributions">Visualizing Prior Distributions</a>
<ul>
<li class="toc-entry toc-h2"><a href="#how-do-we-do-this">How do we do this?</a></li>
<li class="toc-entry toc-h2"><a href="#what-do-we-check">What do we check?</a></li>
</ul>
</li>
</ul><h1 id="visualizing-prior-distributions">
<a class="anchor" href="#visualizing-prior-distributions" aria-hidden="true"><span class="octicon octicon-link"></span></a>Visualizing Prior Distributions</h1>

<p>Dear Young Tim,</p>

<p>My last post to you was about priors for neural networks.
Far from being out of the blue, that post was a product of two active streams of research.
Specifically, I referenced deep learning in computer science and prior specification from statistics.
In doing so, I made two main points.
First,  neural networks are a flexible way to parameterize one’s likelihood function.
And second, we should complement neural network parameters with prior distributions.
These priors provide regularization and enable posterior inference / uncertainty estimates for the parameters.</p>

<p>In this post, I’m going to focus on the statistician’s perspective.
The broader stream of literature in statistics has converged on the following two points.
First, instead of being uninformative as intended, vague / diffuse priors can be highly informative about functionals of one’s model, in ways that one may actively dislike.
Second, the way to understand one’s prior is by spending a lot of time inspecting (i.e., visualizing) the prior predictive distribution.</p>

<p>This post will demonstrate the two points above by way of pictures and an example.
For relevant reading that provides much of the inspiration for this post, see the following papers:</p>
<ul>
  <li>Seaman III, John W., John W. Seaman Jr, and James D. Stamey. “Hidden dangers of specifying noninformative priors.” The American Statistician 66, no. 2 (2012): 77-84.</li>
  <li>Gelman, Andrew, Daniel Simpson, and Michael Betancourt. “<a href="https://www.mdpi.com/1099-4300/19/10/555/pdf">The prior can often only be understood in the context of the likelihood.</a>” Entropy 19, no. 10 (2017): 555.</li>
  <li>Gabry, Jonah, Daniel Simpson, Aki Vehtari, Michael Betancourt, and Andrew Gelman. “<a href="https://rss.onlinelibrary.wiley.com/doi/pdf/10.1111/rssa.12378">Visualization in Bayesian workflow.</a>” Journal of the Royal Statistical Society: Series A (Statistics in Society) 182, no. 2 (2019): 389-402.</li>
</ul>

<p>The conclusion for one’s own research is that when building models of any kind, one should spend time investigating and setting informative priors for one’s models by visualizing the prior predictive distribution.</p>

<p>Below, I’ll share some examples of such prior investigations using the widely available Boston Housing data set:</p>
<blockquote>
  <p>Harrison Jr, David, and Daniel L. Rubinfeld. “Hedonic housing prices and the demand for clean air.” (1978).</p>
</blockquote>

<p>The model that we’ll consider is a <a href="https://www.tandfonline.com/doi/abs/10.1080/03610920500476234">folded-logistic</a> regression model where the location is linearly parameterized based on the dataset’s raw features, and where the scale parameter is homogenous and explicitly estimated.
Note that if <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>Z</mi></mrow><annotation encoding="application/x-tex">Z</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.07153em;">Z</span></span></span></span> follows a logistic distribution with location <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>μ</mi></mrow><annotation encoding="application/x-tex">\mu</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.19444em;"></span><span class="mord mathdefault">μ</span></span></span></span> and scale <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>σ</mi></mrow><annotation encoding="application/x-tex">\sigma</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.43056em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.03588em;">σ</span></span></span></span>, then <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">∣</mo><mi>Z</mi><mo stretchy="false">∣</mo></mrow><annotation encoding="application/x-tex">\lvert Z \rvert</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">∣</span><span class="mord mathdefault" style="margin-right:0.07153em;">Z</span><span class="mclose">∣</span></span></span></span> follows a folded logistic distribution.
Basically, this is a linear regression where we restrict the support of the outcome (i.e, the median home value) to positive real values, following a logistic distribution bounded below by zero.</p>

<h2 id="how-do-we-do-this">
<a class="anchor" href="#how-do-we-do-this" aria-hidden="true"><span class="octicon octicon-link"></span></a>How do we do this?</h2>
<p>The general strategy for prior predictive checks is as follows.</p>
<ol>
  <li>Specify a prior distribution for all parameters in one’s model.</li>
  <li>Sample parameters from the prior.</li>
  <li>Simulate outcomes from the model, conditional on one’s sampled parameters.</li>
  <li>Visualize select statistics based on the simulated outcomes and explanatory variables.</li>
</ol>

<p>To streamline this process, we need an interactive way of specifying our priors.
While many options for interactivity exist, I’ll make use of the <a href="https://www.streamlit.io/">Streamlit</a> package in Python.
This package is easy to use, is generally free of “gotchas,” and enables one to set up a user-interface for one’s programs in a short time.</p>

<p>The basic idea is that we’ll select distributions via drop-down menus, and we’ll select parameters for those distributions via sliders.
Then, once we’ve made our selections, we can press “Draw” or some other descriptively named button.
This button press will trigger execution of a program that we’ve written to sample from our prior predictive distribution and create the desired visualizations.
Finally, we’ll use Streamlit to automatically display these visualizations.</p>

<h2 id="what-do-we-check">
<a class="anchor" href="#what-do-we-check" aria-hidden="true"><span class="octicon octicon-link"></span></a>What do we check?</h2>
<p>For an example of the prior predictive checking process in action, see the following repository: https://github.com/timothyb0912/prior-elicitation/.
In particular, the streamlit app that I created to visualize the prior predictive checks is <a href="https://github.com/timothyb0912/prior-elicitation/blob/master/scripts/app.py">here</a>, and the file I created to simulate from and visualize the prior is <a href="https://github.com/timothyb0912/prior-elicitation/blob/master/notebooks/_01_tb_test_model_classes.py">here</a>.</p>

<p>To begin, we’ll need to choose some quantities to visualize.
Here, we have complete freedom.
For a variety of quantities that may prove useful for prior (or posterior) predictive checks, see</p>
<blockquote>
  <p>Brathwaite, Timothy. “Check yourself before you wreck yourself: Assessing discrete choice models through predictive simulations.” arXiv preprint arXiv:1806.02307 (2018).</p>
</blockquote>

<p>For this example, we’ll consider a statistic that was not described in my paper.
Specifically, we’ll examine the 25th percentile of Y * X, where X is a selected column from our design matrix.
For instance, if we select the column of one’s in the design matrix, then we’ll view the prior predictive distribution of the 25th percentile of Y.</p>

<p>Here’s a picture using a typical diffuse / non-informative prior.
<img src="/blog/images/diffuse_prior.png" alt="Prior Predictive Check of Diffuse Prior" width="680px">
Each coefficient for the design matrix has mean zero and standard deviation of 5.
The scale parameter has a folded normal prior distribution with location 1 and scale of 1 (yes… meta, I know).
From the image, we see that the prior predictive distribution places far too much mass on entirely implausible values (e.g. smaller than the minimum observed median home value or larger than the maximum observed median home value).
The non-informative prior on the model parameters is actually quite opinionated (in misrepresentative ways) about our prior beliefs in functionals of our data.</p>

<p>From here, the idea is clear enough: play around!
Iteratively adjust the sliders and display your chosen prior predictive checks for each new specification.
With enough experimentation, one should be able to create a prior that appears reasonable.</p>

<p>See below for a video clip of how one can use the interactive interface to experiment with different prior settings.
<img src="/blog/images/prior_predictive_app.gif" alt="Web Interface for Prior Predictive Checks" width="680px"></p>

<p>In future posts, I’ll discuss how to set priors programmatically.
Clearly, the manual approach described above will not scale to high dimensional settings.
We’ll need to do something smarter!</p>

<p>P.S. The test statistic chosen for the prior predictive check has some rationale to it.
In a standard linear regression, the first order conditions that one must solve to achieve a gradient of zero is $(Y - \hat{Y})X = 0$, where X is one’s design matrix.
This is equivalent to saying that the mean of <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>Y</mi><mi>X</mi></mrow><annotation encoding="application/x-tex">YX</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.22222em;">Y</span><span class="mord mathdefault" style="margin-right:0.07847em;">X</span></span></span></span> should equal the mean of <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mover accent="true"><mi>Y</mi><mo>^</mo></mover><mi>X</mi></mrow><annotation encoding="application/x-tex">\hat{Y}X</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.9467699999999999em;vertical-align:0em;"></span><span class="mord accent"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.9467699999999999em;"><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.22222em;">Y</span></span></span><span style="top:-3.25233em;"><span class="pstrut" style="height:3em;"></span><span class="accent-body" style="left:-0.25em;">^</span></span></span></span></span></span><span class="mord mathdefault" style="margin-right:0.07847em;">X</span></span></span></span>.
In other words, the observed mean of <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>Y</mi><mi>X</mi></mrow><annotation encoding="application/x-tex">YX</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.22222em;">Y</span><span class="mord mathdefault" style="margin-right:0.07847em;">X</span></span></span></span> should equal the predicted <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mover accent="true"><mi>Y</mi><mo>^</mo></mover><mi>X</mi></mrow><annotation encoding="application/x-tex">\hat{Y}X</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.9467699999999999em;vertical-align:0em;"></span><span class="mord accent"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.9467699999999999em;"><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.22222em;">Y</span></span></span><span style="top:-3.25233em;"><span class="pstrut" style="height:3em;"></span><span class="accent-body" style="left:-0.25em;">^</span></span></span></span></span></span><span class="mord mathdefault" style="margin-right:0.07847em;">X</span></span></span></span> after estimating one’s model.
Checking that the chosen test statistic of a selected percentile (e.g. 25th percentile) of <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>Y</mi><mi>X</mi></mrow><annotation encoding="application/x-tex">YX</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.22222em;">Y</span><span class="mord mathdefault" style="margin-right:0.07847em;">X</span></span></span></span> resembles the distribution of simulated <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>Y</mi><mtext>sim</mtext></msub><mi>X</mi></mrow><annotation encoding="application/x-tex">Y_{\textrm{sim}}X</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.83333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.22222em;">Y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31750199999999995em;"><span style="top:-2.5500000000000003em;margin-left:-0.22222em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord text mtight"><span class="mord textrm mtight">sim</span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mord mathdefault" style="margin-right:0.07847em;">X</span></span></span></span> is a related activity.
This helps ensure that more points of the distribution of <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>Y</mi><mi>X</mi></mrow><annotation encoding="application/x-tex">YX</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.22222em;">Y</span><span class="mord mathdefault" style="margin-right:0.07847em;">X</span></span></span></span> are well represented by one’s model, beyond the mean of the distribution.</p>

  </div><!-- from https://github.com/utterance/utterances -->
<script src="https://utteranc.es/client.js"
        repo="timothyb0912/blog"
        issue-term="title"
        label="blogpost-comment"
        theme="github-light"
        crossorigin="anonymous"
        async>
</script><a class="u-url" href="/blog/simulation/2020/08/31/Prior-Visualization.html" hidden></a>
</article>
      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/blog/"></data>

  <div class="wrapper">

    <div class="footer-col-wrapper">
      <div class="footer-col">
        <p class="feed-subscribe">
          <a href="/blog/feed.xml">
            <svg class="svg-icon orange">
              <use xlink:href="/blog/assets/minima-social-icons.svg#rss"></use>
            </svg><span>Subscribe</span>
          </a>
        </p>
      </div>
      <div class="footer-col">
        <p>A personal blog.</p>
      </div>
    </div>

    <div class="social-links"><ul class="social-media-list"><li>
  <a rel="me" href="https://twitter.com/timothyb0912" target="_blank" title="twitter">
    <svg class="svg-icon grey">
      <use xlink:href="/blog/assets/minima-social-icons.svg#twitter"></use>
    </svg>
  </a>
</li>
<li>
  <a rel="me" href="https://github.com/timothyb0912/" target="_blank" title="github">
    <svg class="svg-icon grey">
      <use xlink:href="/blog/assets/minima-social-icons.svg#github"></use>
    </svg>
  </a>
</li>
</ul>
</div>

  </div>

</footer>
</body>

</html>
