{
  
    
        "post0": {
            "title": "Heavy on the graphs",
            "content": "Heavy on the graph . Dear Young Tim, . We both know how we hate the feeling of constant pressure behind our ears— the radiating and throbbing throughout our skull, our sunken head, and our collapsed shoulders. At those all too common times, our unplanned bodily tension mirrors and points to the deluge of thoughts racing through our mind, sweeping us off our feet and taking our breath away, for good or for ill. This is overwhelm embodied, and it IS a problem. Most basically, chronically elevated stress harms and kills. . So, what are the underlying issues leading to this overwhelm? Centrally, it’s information overload: I have too many sources of input (ESPECIALLY from my own thoughts1) relative to the effectiveness of my current methods for making sense of, remembering, and leveraging this incoming information. Similarly, problem overwhelm heavily overlaps information overwhelm. When there are many problems, I have difficulty holding all of them (and their relevant information) in my head at once for joint consideration, analysis, and prioritization. . Informational overload is not a new experience for us, though its intensity may be greater now than in the past. Most recently, I noted this problem in January’s post, where I suggested writing out our problems, so they could be held on paper rather than in our minds. In similar fashion, a year ago, I suggested writing out our thoughts in general. In both such cases, writing has been helpful but insufficient. We transformed our source of overwhelm from our thoughts to written documents. However, these documents still have to be read, processed, and internalized or utilized. . Fundamentally, we need more than another information source or medium. We need a way to organize the information we have, to make sense of what we know of the world. We need a way to make clear what we know, to convey the big picture to ourselves, and to help us retrieve desired information on command in the future. What we need is a map of our knowledge, or more precisely, many such maps. In much the same way that a series of geographical maps (at different zoom levels and for differing purposes) organizes the complexity of the real world and makes it legible enough for us to productively navigate it, a series of mind maps can help us organize and navigate the mental landscape of our thoughts. . Of course, one might ask why do we need to organize our thoughts? Why is capture plus digitally-enabled search and need-based usage not enough? There are at least three reasons why organizing our mind(‘s thoughts) is useful, beyond capturing and searching. First, in the simplest of terms, we want to be mentally ordered, not mentally disordered. This helps us act effectively to meet our needs. The process of organizing our mind IS bringing order to our mind. . Secondly, great organization comes at great cost, but it brings great speed. See the notion and organizational principle of mise en place, used by world-class chefs to enable the rapid production of complex meals. As stated in December 2020 one of our life goals is to move fast forever, and this requires that we think fast too, hence the wisdom of organizing our mind. . Finally, we want not just to think quickly and to be able to find information when needed. After all, it becomes annoying to open one’s phone or computer to search repeatedly, even if we can find what we’re looking for. It is much more efficient to simply know or remember something. Here, mind maps are also helpful, as they engage our visual and spatial memories, in addition to our verbal memory. . So, how can we become mentally organized? At its core, I believe we can mentally organize ourselves as follows. (Note, I believe that self-education will generally include these steps as well). . First, think externally (i.e. on paper and / or on computer). This aids storage, retrieval, and editing of one’s thoughts. | Secondly, think visually and relationally, in addition to verbally. That is, make graphical diagrams—images consisting of nodes and edges, such as mind maps, concept maps, and the like. I’ll refer to all such images in this post as mind maps. This is where we make sense of how our knowledge relates to other information, where one summarizes information for comprehension, and where one can place images that aid your future recall. | Finally, think repeatedly and stably. Internalize our mind maps by repeatedly recalling portions of the map, such as via image occlusion with Anki. This will ensure we have mental access to the mind map and all portions of it, alongside the underlying knowledge it contains. | . After these three steps we will likely find ourselves at peace with whatever information we just mapped. . So what does an example of such mental organization look like? Consider this blog as a collection of letters, filled with advice that I hope is useful, to both younger and future me. Here, as in all cases, I need to know what information is in the blog in order to benefit from it2. The mind map below depicts one way of organizing this blog’s posts (except for this post itself, which is a descendant of the title node). Please do open the image in another tab to explore a larger version of it. . While I plan to detail this mind map in future posts, I will state the high level story here. In the beginning, we started off discussing data science and our thoughts on its practice for quantitative learning. Then we shifted to thinking about processes for qualitative learning, and our focus has been alternating with problem-solving in general. There will be more to come on this in the future, but from our outline of a story about the blog so far, we can further discuss the nodes in each story (or mind map) section, and we can discuss how each post impacts our understanding and our actions. . Hopefully this example gives an idea of what is possible, and I hope this and many other mind maps help us reduce our incidence and intensity of information overload. . Till next time Young Tim, . Other sources of input include sensory information from external events, books, the internet, and people I interact with. &#8617; . | Imagine a section of articles on Wikipedia that I don’t know exists. I get as much benefit from it as I do if that section of articles did not exist at all. &#8617; . |",
            "url": "https://timothyb0912.github.io/blog/psychology/practice/2022/04/30/Graph-Heavy.html",
            "relUrl": "/psychology/practice/2022/04/30/Graph-Heavy.html",
            "date": " • Apr 30, 2022"
        }
        
    
  
    
        ,"post1": {
            "title": "Questions for Introspection: Part 2",
            "content": "Questions for Introspection: Part 2 . Dear Young Tim, . Happy end of the first quarter of 2022! . Being a time of endings and new beginnings, this seems a good time to close the loop on our posts from September and October, 2021. To bring us all back up to speed, the scene is as follows. . We’re often trying to self-improve. The process for this improvement involves introspecting, to make clear and then make right our beliefs, wants, needs, feelings, and behavioral patterns. The graph below depicts how the various elements of our introspection relate to each other. Beyond illustrations, it is helpful, especially in emotionally charged moments, to have a list of questions to ask about each introspection element. . Our post from October 2021 provided such a list for the “when” and “then” nodes in the graph above. Today’s post provides a list for our remaining introspection graph elements, the “given” nodes. These elements cover the parts of our story that give context for our behavioral decisions and feelings. . As before, these questions are stated without introduction, since the elements of our introspection graph were described previously, in September 2021’s post. These questions are meant to guide our “meditating” or thinking deeply about our role in our problems. . Needs and emotional wounds . “Referring to the lists1 of emotional wounds we carry, which wounds are triggered by our current situation?” | “Which past experiences/wounds are affecting which parts of me (behaviorally)?” | “Referring to our lists of needs, what needs does each of our activated parts have in this situation?” | “Beyond the pre-identified needs, is there anything else we need in this situation? Try to think of things which would bring satisfaction, beyond just resolving the current situation.” | “How can we give the parts of us with emotional wounds, some of what they need right now?”” | . Goals . “How do I wish to respond to problems, in general?” | “What do I most want or desire right now?” | “What do each of our activated parts want?” | “Based on our needs and beliefs, what were our goals or desires, coming into this situation?” | “What are my goals? (what/why/how)” | “How can I reframe my goals / failure-states / problems in such a way as to alleviate the undesired outcomes or consequences (including undesired feelings or emotions)?” | “If our goals are reachable given some series of our own actions, how can we substantively and immediately progress towards them?” | . Failure states . “Based on my emotional wounds and needs, what ‘reasonably likely’ outcomes do I most wish to avoid?” | “What am I most afraid of right now?” | “What are my definitions of failure?” | “How can I reframe goals / failure-states / problems in such a way as to alleviate the undesired outcomes or consequences (including undesired feelings or emotions)?” | “If our failure steps are avoidable given some series of our own actions, how can we substantively and immediately guard against or prevent failure?” | . State beliefs . “Given our goals, failure states, needs, and emotional wounds, what did we believe about our state coming into this situation?” | “Given our goals, failure states, needs, and emotional wounds, what did we believe about our external circumstances in this situation?” | “What do each of the activated parts believe about our overall state?” | “What fear about the situation did each part of you start with?” | Disputation questions from cognitive behavioral therapy (per belief): “What is the evidence for this belief?” | “What are alternative-viewpoints / statements of this belief?” | “What is the usefulness of this belief?” | “What common thinking errors and biases might this belief be partially formed by?” | “What can I do to take action based on this belief (or its restatement/refutation) right now?” | . | . Outcome beliefs . “Given our goals, failure states, needs, and emotional wounds, what prior beliefs did we hold about potential outcomes of our situation? E.g. outcome X is unlikely, or outcome Y should not occur because Z.” | “Given our goals, failure states, needs, and emotional wounds, what prior beliefs did we hold about potential outcome consequences? E.g. if outcome X occurs, then it means Y or Z will happen.” | “What did each of the activated parts believe about our potential outcomes?” | “What fear about the potential outcomes did each part of you start with?” | Disputation questions from cognitive behavioral therapy (per belief): “What is the evidence for this belief?” | “What are alternative-viewpoints / statements of this belief?” | “What is the usefulness of this belief?” | “What common thinking errors and biases might this belief be partially formed by?” | “What can I do to take action based on this belief (or its restatement/refutation) right now?” | . | . Pre-decision feelings . “What do I feel, even now, when thinking of my state beliefs above?” | “What parts of us are or were activated when thinking of our state beliefs? Where in the body is or was this activation felt?” | “Which, if any, parts of us are in conflict before our decision, and why?” | “How could we have, or how might we in future similar situations, give the parts of us with emotional wounds some of what they need before decision-making?” | “What feelings do we wish we channeled before our decision?” | “In the future, how can I activate the parts of me with the emotional and behavioral traits I wish to display/use in this situation?” | “In this situation, how could I switch emotional states altogether, from undesired state to desired state, e.g. fear -&gt; excitement or anger?” | . Such lists might be generated from the Self-Therapy Journey quizzes or curated from their overall list and similar ones. &#8617; . |",
            "url": "https://timothyb0912.github.io/blog/psychology/practice/2022/03/29/Questions-for-introspection-2.html",
            "relUrl": "/psychology/practice/2022/03/29/Questions-for-introspection-2.html",
            "date": " • Mar 29, 2022"
        }
        
    
  
    
        ,"post2": {
            "title": "Lemonade from lemons",
            "content": "Lemonade from lemons . . Problem . Dear Young Tim, . Looking back over the past year, procrastination has been our constant companion and problem. For this discussion, we’ll define procrastination as either of the following two conditions. Condition one: having a plan for our time, yet hesitating to do what we planned, AND feeling that such hesitation was not in our long-term best interest. Condition two: hesitating to make a plan for our time, AND feeling that such hesitation was not in our long-term best interest. . Given the definition above, procrastination can be viewed as a detrimental practice, to be stopped as soon as possible. To this end, one straightforward strategy is to try to stop hesitating on one’s planned tasks or on planning. . However, pause briefly and imagine a complementary way. How could we respond if we viewed our hesitation as having a positive intent? Per our definition above, a second strategy to ‘stop procrastinating’ is to take action such that our hesitation still supports our long-term best interests. Then hesitation would be free of guilt and detrimental qualities; it would not be procrastination. Practically, how can we design a set of actions that together transform our hesitation from procrastination into production for our long-term interests? . Most comprehensively, our problem can be stated as follows. Given that we sense many aspects of our physical and social environment, when we interpret one or more of our sensations as indicating our unmet needs, then we commonly feel intense emotional activation, and we hesitate to do that which we planned or we hesitate to plan. Meanwhile, we anxiously observe the racing of our polarized thoughts. Finally, we then feel secondary guilt for ‘spending’ our time with nothing to show for it, i.e. acting against our long-term best interests. . Of course, despite our actions, we prefer to avoid feeling anxiety or guilt, if possible. What we really want or need is a way to productively channel our thoughts and feelings, if and while we hesitate, so that we identify, assess, and eventually satisfy our unmet needs. THAT would be in our long-term best interests. . Solution . So, what can we do? One hypothesis is that if we use communication templates, then we will increase the percentage of time that hesitation leads to production rather than procrastination. The idea is that we can design the template with cues for information we need to contextualize and respond to the problems that motivated our hesitation. Then, instead of being lost in thought while procrastinating, we can combine our thoughts with the template, crafting a story that serves as a record of the problem for ourselves and anyone else who might help us. We can write, type, or speak our message based on the template, and we can store the resulting document / file for later. . What might such a template look like? As a useful example, consider the following template for “emotional problem-statements”: . I observe that GIVEN {{these-circumstances}} WHEN {{this-happens}} THEN [facing {{these-concerns}}] I decide to FEEL {{these-feelings}}, as opposed to {{these-other-feelings}}, in order to satisfy {{these-NEEDS}}, while accepting {{these-downsides}}. So far, I have taken {{these-steps}} to fulfill my needs, and for further help, I am REQUESTING {{these-actions}}. . Elements in curly-braces, i.e. ‘{‘ and ‘}’, are to be replaced with our own descriptions. Clauses with rectangular-braces, i.e. ‘[’ and ‘]’, are considered optional and to be included at a communicator’s discretion. . Outcomes . So far, using this template has helped in the following ways. Since adopting it, the majority of times I hesitated to plan or execute my plans, I generated multiple, actionable, requests. Typically, I’ve generated one or more requests per activated inner part, and when hesitating, it’s almost always the case for me that two or more inner parts are activated and polarized. Accordingly, I come away with a collection of requests, and as I slowly fulfill them, I’ve experienced less frequent bouts of hesitation. This helps accomplish the first procrastination reduction strategy of not hesitating so much. . Simultaneously, by producing these records of what happened, what I’m feeling, and what I need and want, I document the mindfulness and emotional regulation work described in therapeutic modalities like dialectical behavior therapy. By creating such templated documents, i.e. by standardizing and tracking emotional data, I bypass feeling the guilt of procrastination, and I instead feel proud of translating my raw emotions into information that can then be acted upon and analyzed. This helps accomplish the second procrastination reduction strategy of making hesitation productive. These emotional problem-statements enter the top of a productivity funnel that starts with problem logging, continues with problem observation, hypothesizing, and experimentation, and then hopefully ends with problem resolution. . Rationale . Beyond the beneficial outcomes observed and described above, how and why do I think using this template should be helpful? What do I think are the mechanisms behind the magic? . At the most basic level, we can ask why we believe templates will help at all. Needing to communicate effectively, especially in the difficult context of multiple heightened emotions, is a common problem that numerous thinkers have addressed. Across fields from architecture to software engineering, design patterns “describe a problem that occurs over and over again in our environment, and then describes the core of the solution to that problem.” Templates are patterns, and they are typically created with purpose. For instance, communication templates solve recurring communication problems. . In particular, the emotional problem-statement template is based on the Non-Violent Communication (NVC) template of Dr. Marshall Rosenburg. In describing the NVC template, Rosenburg writes: . We can apply NVC to resolve the internal conflicts that often result in depression. In his book The Revolution in Psychiatry, Ernest Becker attributes depression to “cognitively arrested alternatives.” This means that when we have a judgmental dialogue going on within, we become alienated from what we are needing and cannot then act to meet those needs. Depression is indicative of a state of alienation from our own needs. . The applicability of this template to our situation is immediate. We noted above that procrastination almost always involves two or more polarized inner parts within us, i.e. a judgmental dialogue. Moreover, in December’s post, we noted that ‘all encompassing procrastination’ is often how our depression manifests itself. Accordingly, the NVC template stands out as a communication pattern directly aimed at the emotional disturbances that drive my procrastination (both moderate and depression-level). . Next, I further combine the NVC template with three other communication patterns. The first additional pattern is the Given-When-Then template for stating one’s obervation of what is happening. This template is frequently used in software design and business to describe a system’s desired behavior after creation of a product. I have also found it useful for describing the current behavior of a system, as is called for in the first step of the NVC template. Since the Give-When-Then template is useful in business when trying to solve people’s problems, my a-priori assumption is that it will remain useful when trying to solve personal problems. . The second additional pattern is the Y-statement template. Here, we follow the research of Dr. Lisa Feldman Barrett, who reminds us that emotions are constructed / predicted. I.e., our emotions are our choice, and we have influence in choosing what emotions we experience. As noted by Rosenburg in his book, Nonviolent Communication: A Language of Life, . NVC heightens our awareness that what others say and do may be the stimulus, but never the cause, of our feelings. We see that our feelings result from how we choose to receive what others say and do, as well as from our particular needs and expectations in that moment. With this third component, we are led to accept responsibility for what we do to generate our own feelings. —emphasis added . Of course, others can cause our feelings to an extent, especially through physical violence. However, the overall message is clear. We all make decisions that impact the emotions we experience. Using the Y-statement template for describing one’s decision-making makes this choice explicit. We also reduce secondary guilt by consciously accepting that we are investing time in order to process the emotions fueling our hesitation. . Finally, the third additional pattern we’ve adopted is a template for asking for help. It follows Jim Rohn’s advice that when asking for help, it’s useful to inform people of what we have done and what we have read/learned about solving our problem. The idea is that we increase our chances of getting help, by helping ourselves. Moreover, as advised on the famous question-asking website, StackOverflow and in its linked-to resources, including what we have unsuccessfully tried helps people understand what differentiates our problem from already solved problems. This increased clarity also helps us secure effective assistance. . Well-wishes . Young Tim, hopefully this post makes it clear: there are great practical and logical reasons to use communication templates like the emotional problem-statement. For years going forward, we hope such tools continue to helps us turn procrastination into production— to make lemonade from the lemons of our emotional disturbances. . .",
            "url": "https://timothyb0912.github.io/blog/practice/psychology/2022/02/28/Lemonade-from-lemons.html",
            "relUrl": "/practice/psychology/2022/02/28/Lemonade-from-lemons.html",
            "date": " • Feb 28, 2022"
        }
        
    
  
    
        ,"post3": {
            "title": "On problems, areas, and needs",
            "content": "On problems, areas, and needs . The lesson . Dear Young Tim, . Happy New Year! Thanks so much for getting us here. . I know it’s 2022, but can we think back for a second to 2008? Back to that primordial “whoop!” we let out in the parking lot? Back to when Darren and Andrew put us on? Back to when they saw us struggling to learn to skid: struggling to unweight the rear wheel. . We took notice when we saw the rain clouds. This was the kind of storm that made people seek shelter. Typical of us, we instead strapped on our rain jackets and rode out. Mobbing in the 7-Eleven parking lot, we ate hotdogs, drank slushies, and skidded circles with freedom that boys fresh out of high school feel. . To be honest, I didn’t want to ride in the rain. I hated the cold and soaking environment. However, where I saw problems, this time—they saw promise: a promise of a path that while harsher in bearable manners, was easier in all the ways I needed. So we weren’t initially fans, but boy did we grow to love it. Once Darren and Andrew hit their first 360’s, we were sold. It was playtime. . And it worked. That first yell—that first glimpse of success. A 270-degree skid in the parking lot, and I was set forever. Our fingers were bone-chilled, but our hearts were warmed! We would skid at will, on any surface, any time. . All this was possible once we knew what success felt like, even if artificially induced. Succeeding at easier versions of a problem (skidding on wet ground) helped us reach success on the standard version (skidding on dry ground). This is a lesson we’ve been taught repeatedly, in differing contexts: mathematics1, bicycling, violin-playing, programming, etc. We often progress rapidly when we give ourselves easy, intermediate versions of our main tasks, to succeed on. . The challenge . One area this truth applies is in our personal problem solving efforts. Young Tim, I’m particularly grateful for your motivated stance towards maintaining a handle on the problems we’re facing. This includes your attempts to follow the suggestion from December’s post: making a list of all our problems. By now, you’ve realized that this task is a large one. In fact, the task is so large it is an obstacle. Here, our earlier lesson can offer some help. . First, I hope you take comfort in how expected it is that you are anxious to stop using anxiety to keep track of all our problems. You have operated in a stressful state beyond your capacity for decades. I would be suspicious if you were NOT anxious to exit this state of being. . My intent in this note is to help us with listing, with getting all our problems out of our mind so we can more easily prioritize and stay aware of them. Following our lesson on solving easier versions of a given problem, I suggest an approach which, although less direct, may be more successful than sitting down to write a single, unstructured list. As it is sometimes said, see transportation joke, sometimes the shortest path is the longer route that’s less traveled. . Here is the problem with December’s advice of listing our problems: we feel overwhelmed because there are way too many problems. Too many to list in one sitting. Too many to hold in our mind at once. Too many to review at once. . If we reframe our listing problem, one challenge we’re facing is how to stay aware of a large number of individual problems, without triggering feelings of overwhelm. Fundamentally, this is a challenge of organizing our issues. . The application . So, if listing our individual problems is hard, what related items can we list more easily? How can these easier lists help us complete the problem list? How can we create intermediate packets? . Two easier and useful lists to create are: . a list of our needs, at an abstract level | a list of the practical areas in our life, that we use to meet our needs | . The diagram below depicts (i) these complementary lists, (ii) our desired problem list, and (iii) relations between them. On the edges are questions that lead from one list to another. For instance, problems indicate examples of how not to manage related areas of one’s life. Similarly, if we ask why are our needs not met, we could trace most answers to one or more problems in one or more of our life’s areas. And so on, so forth, going around the diagram. . Here’s what I believe makes this triangular view beneficial. By projecting from the high dimensional list of problems, to complementary, lower dimensional lists, we can aid our review and planning for problem-solving. For example, I estimate that I have more than 200 problems. However, I list only 12 - 30 needs, depending on how I count. Likewise, when I list the various areas of my life, I have approximately 30. Such magnitudes are much less foreboding, and they are of the same order as “reasonably short” lengths of time– like a month. . Additionally, one can think of problems as undesired outcomes, with respect to our hill-climbing or improvement efforts against the following objective: maximizing satisfaction of our needs, subject to avoiding system failure in any one of our life’s areas. Listing our areas and needs, and connecting them to our problems, helps us stay aware of the underlying components giving our problems relevance. . So how can we reap these benefits? Start by listing the areas in one’s life: reviewing, planning, bicycles, nutrition, health, etc. Then, list the fundamental needs you have in life. Finally, think of the problems you are currently facing. List all that come to mind before you feel intimidated by the number of issues. When you’re done with your initial listing, you can categorize your initial problems into their respective life areas and associate the problems with the (unmet) needs that they relate to. Now, we’ll be able to use the areas and needs as categories, to cue our review and to sequentially list problems in. . Young Tim, I hope this advice helps you create an overall view of our problems. I’m excited to see where we’ll go once we have a fuller view of the terrain! . Till next time: may the storms smooth your path, and where an even younger us may have feared loss of control, may we find joy in life’s most wicked spins. Take them and make them easy! . See, for instance, George Polya–author of the mathematics classic “How to Solve It.” Polya suggests, as one strategy to deal with hard problems, to solve easier but related problems. &#8617; . |",
            "url": "https://timothyb0912.github.io/blog/practice/2022/01/29/Problems-areas-needs.html",
            "relUrl": "/practice/2022/01/29/Problems-areas-needs.html",
            "date": " • Jan 29, 2022"
        }
        
    
  
    
        ,"post4": {
            "title": "Personal Data Science: What, Why, and How",
            "content": "Personal Data Science: What, Why, and How . Introduction . A leader of transformation, and managers involved, need to learn the psychology of individuals, the psychology of a group, the psychology of society, and the psychology of change. . Some understanding of variation [i.e. statistics], including appreciation of a stable system, and some understanding of special causes and common causes of variation, are essential for management of a system, including management of people. —William Edward Deming The New Economics for Industry, Government, Education, 2nd Edition, p. 95. . You’re the only one who can manage yourself, and here’s a key point. No mood expert I know of would ever talk about curing depression. Keep this point from this lecture. The goal is not to cure depression. The goal is to learn how to manage your moods. These are things you have to manage every day of your life, and managing your moods requires the skills— psychologists call it self-regulation, but the simple phrase managing your mood counts. —Michael D. Yapko, Ph.D, Australian Psychological Society How to recover from depression . Dear Young Tim, . Congratulations on making it to the end of 2021! Extra congratulations for making it here without any permanent debilitations, mentally or physically. I wasn’t sure we would see the end of the year in this state, so there is much to celebrate! . Despite this good news, I’d like to offer you an apology. Looking from hindsight, I did not protect us and meet our needs. Accordingly, you and your twin protector of all-encompassing-procrastination depression are fully active. I’ve left you two running about, trying desperately to hold onto ourselves and the few remaining goals we’ve carried to the end of the year. . In all honesty, relying only on our mind’s awareness and on our rotating present attention to keep track of things, has left us exhausted and in shambles. It has left us in a severe local minima. Young Tim, in our hero’s journey, we’re at the point of having begun the journey, having achieved some success, but nonetheless having come to hard times. Now, it’s time to change in order to experience better times in the future. . Part of this change will involve seeing therapists and coaches. They’re absolutely helpful. However, we nevertheless face the decision of “how will we interact with mental health professionals?” As statistician Talithia Williams so aptly stated, . Medical doctors [and mental health workers] are experts in the population, but you are the expert on yourself. And so when two of you come together, when two experts come together, the two of you are able to make a better decision than just your doctor alone. . In a real sense, we need to be head of “personal data science” for our own mental health management. There are good economic reasons for this. For one, I have the greatest competitive advantage for this role. I have direct access to what is going on in my mind at any time. I can perform the data collection whereas others cannot. I am also a professional data scientist. Who is better suited to analyze my data productively, quickly, and thoroughly? . Finally, this data science for mental health role has a huge gap. The role is not filled comprehensively or well by the mental health professionals I have interacted with. Put another way, no one in my life, neither I nor anyone else, has been using knowledge of how my personal mood systems vary in combination with principles from psychology for the management of my moods. I suspect that I can play this role better than others, and since it is not being done by others, I may benefit from doing it. . What is personal data science? . This discussion brings up the question of what is “personal data science,” before we go too far down the rabbit hole of “why may I want to do this?” The basic idea is that personal data science is the application of data science perspectives and techniques to the betterment of personal problems that one is facing, with the aim of leading or managing oneself more effectively. Note this includes using thick qualitative descriptions and story elicitation, as well as using quantitative data collection and analysis. See the image below for a full overview. . . As mentioned earlier, one immediately relevant personal problem is the management of our emotions and moods on a daily basis such that we accomplish our goals. Another related personal problem involves ensuring that our exercise and nutrition maintain our good health over the long-term. Finally—but not comprehensively—we might ask how can we use physical health outcome data such as our temperature, our blood pressure, our blood glucose levels, and our heart rate? If we collect and store our observations—our quantitive stories— about these important topics, then we may be able to use qualitative and quantitative analysis methods on the collected data. . Why should we maintain personal data science projects? . Now that we know what personal data science is, why should we spend our limited and precious time doing it? As mentioned in the introduction, we have a competitive advantage in this subject. However, the case for practicing personal data science is deeper than this: . . Young Tim, “as knowledge workers, our mood is one of the largest determinants of our productivity” (Tiago Forte). This is true whether we are doing work to better our own lives or for others. Each of us is affected by our own moods more frequently than any other person. Therefore, each of us bears the highest responsibility for managing our own moods. . As quoted by W. Edwards Demings—a pioneer of modern American management—, management requires knowledge of both psychology and of variation (i.e., statistics). Essentially, everyone needs to be their own data scientist, with subject matter expertise in psychology. . Of course, if we take up this role, benefits will come our way. We’ll gain increased self-knowledge by paying more consistent attention to ourselves; we’ll be able to exercise greater self-control through our increased self-knowledge; and we’ll be able to achieve greater quality of our own lives in the future— we’ll be better able to thrive on our own as demand for therapists far outpaces the supply of therapists (both in absolute number and in quality hours). . In addition to immediate and future benefits to our own lives, we can help enable future benefits for other people. If we are able to execute useful personal data science projects then we may be able to make helpful tools and distill useful stories for our families, our social circles, and the public. . Finally, on a personal and practical level, if we make everything a data science project, then we gain . immediate calm because these projects are familiar to us; | immediate clarity, because we have reasonable, default project starting points; | immediately greater courage from our history of long-term data science success despite adversity; | future analytical benefit from accurately storing and retrieving data as opposed to inaccurately reconstructing memories; and | future analytical benefit from collecting pieces of externalized thought that can be more easily manipulated than they could be in our working memory. | . How to get started? . Given these benefits, the immediate question is–how can we get started? I believe the thing to do is to leverage Susan Collins observation that “emotions are data.” As far as personal data science goes, a critical data science project to begin with is collecting data on and understanding our emotions and moods. The reason for this is that if we feel bad, then our attempts to solve any other problem become more difficult. . To get started, focus on infrastructure basics. Specifically, consider the “golden path” shown in the diagram below. . . Start by creating a digital folder. Let this folder house all of our data science projects, and yes, there will be multiple. . A helpful structure for our overall folder could be the P.A.R.A. folder system. That is, four sub-folders: projects, areas, resources, and archives. Each of our data science projects will live in its own repository within one of these four sub-folders. . Next, in areas/, make a repository for the data science project of managing one’s moods and emotions. In this repository, we’re going to track how we’re feeling throughout the day and across days. Some guidance on how to perform emotion or mood tracking is described here. See also the moodmeter app and process. . An easy way to set up the repository is to use the cookiecutter folder template that I set up for data science projects. We’ll also want to initialize software for version controlling all of our files, such as git for code or files we may wish to frequently change and git-annex for media files. This will get us to a state where we have a local “home” on our computer for our work. . On a mental level, our first goal will be to understand how our moods fluctuate, and in relation to what circumstances. Our second goal will be to use this knowledge to regulate the moods and emotions we experience, aiming to increase the presence of “Self-emotions”: the 8C’s and 5P’s. My rationale is that doing so maximizes our chance of achieving our goals. In particular, we’ll want to leverage our emotions to support our health in all the ways we can: physically, mentally, emotionally, socially, romantically, and financially. . To do so, after we’ve made an emotion or moods repository, make a note listing all the problems that we can think of: problems that either stem from or cause our emotional dysregulation. This will overview the problems that we are dealing with— i.e., the full-set of problems we’ll be transforming and that data science might help us reduce. . After the setup described so far, we’ll have a local but insecure folder for all of our personal data science work. We’ll want to take additional steps to secure our data both on our computer locally as well as on external hardware and over the internet. The full setup we’ll use is illustrated in the golden path diagram above, but we’ll describe it in-depth in future posts. . Young Tim, if you make this turn towards organizing everything in your life as personal data science projects, then I think you’ll be able to put yourself back in order again. And what a fun ride it’ll be. Fear not, for you’re well prepared for this role: more prepared than for any other thing we’ve done to date, and I’m excited to see what good it amounts to. . Till next time, . P.S.: For others’ excellent takes on why personal science and what we can gain, see the following peer reviewed publications: . The Unreasonable Effectiveness of My Self-Experimentation by Seth Roberts (2010) | A Conceptual Framework for Personal Science by Gary Isaac Wolf and Martijn De Groot (2020) | From self-tracking to self-expertise: The production of self-related knowledge by doing personal science by Nils B. Heyen (2020) | .",
            "url": "https://timothyb0912.github.io/blog/philosophy/psychology/practice/2021/12/25/Personal-data-science.html",
            "relUrl": "/philosophy/psychology/practice/2021/12/25/Personal-data-science.html",
            "date": " • Dec 25, 2021"
        }
        
    
  
    
        ,"post5": {
            "title": "On the importance of stories",
            "content": "On the importance of stories . When my story’s told, how will they tell it? Will they say I was a giver or remember I was selfish? Will they say I was a sinner or pretend I was a saint? Will I go down as a winner, what’s the picture they gon’ paint? Wouldn’t say that I’m a quitter, that’s one thing I know I ain’t. . J. Cole, “Farewell” | . What’s in a worldview? . Dear Young Tim, . What do you think a worldview is, and how does it relate to data science? Psychologist Mark Koltko-Rivera defines a worldview as one’s . set of beliefs and assumptions that describe reality […]. [I.e.,] the interpretive lens one uses to understand reality and one’s existence within it.” . We all have a worldview (or worldviews), and worldviews fundamentally involve data science practices, such as modelling. . In particular, understanding reality includes procedurally understanding reality. That is, understanding reality includes understanding concepts such as “if I do X, then Y will occur.” Or, “if I see puddles all throughout the street, then it likely rained.” . Decision trees and decision lists from machine learning show that such knowledge is actually a model of the world. Though usually implicit, this model is nonetheless a statistical model. Like all models applied to the real world, the models that make up our worldview are built on data from our experience and/or on instruction we have received from others–i.e., on prior information. . Prior information about procedural knowledge is what one receives when a dentist says “if you don’t remove your wisdom teeth, then they will harm your molars.” Prior information about procedural knowledge often takes the form of an event, caused by preceding events and causing following events thereafter. That is to say, a story. Stories are procedural knowledge, organized and communicated from one person to another (including from one to one’s future self). As such, stories have served a tremendous educational role throughout human history. . Point 1: The important thing to realize is that we’re all data scientists who build and rely on wordviews / stories / models-of-reality. We differ in how explicit, how formally, and how painstakingly we (re)construct them. . Trauma and revision . Trauma . If we all go through life constructing and relying upon worldviews, then what happens when our ways of understanding the world are incorrect? Well, when we painfully discover our errors in understanding, this is trauma. . As put by psychologist Ronnie Janoff-Bulman in “Assumptive Worlds and the Stress of Traumatic Events: Applications of the Schema Construct”: . In the case of traumatic negative events, individuals confront very salient, critical “anomalous data,” for the victimization cannot be readily accounted for by the person’s preexisting assumptions (p. 121). . Simply, trauma raises our awareness1 of what is wrong— with the world and with our worldview. Trauma calls us to consider that what we previously believed about the world may have been incorrect. Our previous stories, models, and worldviews have given incorrect predictions, and now we have painfully suffered the consequences of these errors. The question becomes, what will we do in the wake of these consequences? . Worldview revision . What is likely to happen first is that we will withdraw. We will seek safety to rest and reassess our situation. . Once beyond immediate harm, we come to a fork in the road, with at least two options. Will we double down on our existing beliefs, despite the new information? For instance, will we believe that “sometimes these things just happen?” Or, will we revise our worldview, models, and stories? Will we create a new understanding of the world that (i) explains our past experiences, (ii) explains the traumatic experience, and (iii) offers us hope for a satisfactory or desirable future? . Note, this point of divergence has been identified by writers before. As described by psychologist Janoff-Bulman above, “the coping task of the victim” is that . [v]ictims must rework the new [traumatic experience] data so as to make it fit and thereby maintain their old assumptions, or they must revise their old assumptions in a way that precludes the breakdown of the entire system and allows them to perceive the world as not wholly threatening (p. 121). . Here we see possibility for three options: . maintain our predictively unsuccessful worldview despite conflicting data, | adopt a new and predictively successful worldview, or | adopt a new and predictively unsuccessful worldview– leading to system breakdown in the face of a terrifying world that we cannot predict. | . In a business context, Andrew Grove (former CEO of Intel) makes similar observations. He says: . a strategic inflection point is a time in the life of business when its fundamentals are about to change. That change can mean an opportunity to rise to new heights. But it may just as likely signal the beginning of the end. . He goes on to say that . it’s exactly at [a strategic inflection] point that the nature of the business around us is changing. And it’s changing in a subtle way, but changing in a profound and lasting fashion. And those people in a business who do not make the adjustment to the new [conditions] go into decline. . Note that traumatic experiences may indicate a strategic inflection point in our life’s business. It may be that the nature of how our world works has indeed changed, thus leading to incorrect predictions and problems from our current worldview. . Lastly, as noted last year by novelist Arundhati Roy, with generalizing edit by myself, . Historically, pandemics [i.e. traumas] have forced humans to break with the past and imagine their world anew. This one is no different. It is a portal, a gateway between one world and the next. . We can choose to walk through it, dragging the carcasses of our prejudice and hatred, our avarice, our data banks and dead ideas, our dead rivers and smoky skies behind us. Or we can walk through lightly, with little luggage, ready to imagine another world. And ready to fight for it. . Point 2: Whether our stories about the world are incorrect because the world changed or because our tales were inaccurate all along, the way we edit our stories in the wake of trauma is critical. . Let’s make time to hone our stories. We can either invest the time in growing by revising our worldviews to accurately accommodate our new information, or we can suffer increasingly, with unchanging or inaccurately changing stories (relative to reality). . Writing anew . If we want to revise our stories and models of how the world works, the next questions are how should we revise them and to what ends? . Here, it helps to understand the wide breadth of story forms that exist. As noted earlier, stories play an educational role, and this role persists across disciplines with vastly different contexts. As a result, we get a wide range of story types that we can use. . Given that we’re trained in engineering, starting with stories in quantitative research is no surprise. Essential reading in this area includes Andrew Gelman and Thomas Basbøll’s ‘When Do Stories Work? Evidence and Illustration in the Social Sciences.’ Here, there is an old connection where causal graphs are used to ‘tell causal stories’ (p.2) and where stories are represented by causal graphs. Accordingly, one method of worldview revision is to construct causal graphs that represent our understanding of our world’s important systems and to then assess and modify the graphs. For more details, see the following article in preparation by my colleagues and I. . Turning to qualitative research, we find narrative, text-based stories as opposed to graphical and statistical stories. Here, there are examples such as ‘Ethnography as storytelling’ in anthropology and sociology or the stories compiled by journalists and biographers. Particularities aside, imagine a researcher coming to revise an existing story: to write a new journal article on an old topic, to write a new biography on an already documented individual, or to write a new ethnography on a culture that has been studied by others. In all instances, the story-revision process may have high-level similarities. . Specifically, one would likely: . study what stories have already been told, especially for errors or anomalies | increase one’s self-awareness / mindfulness as one observed and documented all that one could about the story topic now, | become self-reflective in organizing and making sense of one’s gathered facts | become self-expressive in communicating one’s new understanding / story. | . This editing process maps directly to the psychological optimization procedure I described in May: we have education, introspection, and then integration. . Indeed, psychotherapy is where story editing meets our current interests. Here, we see clients tell their life’s story to therapists or to their journals. This is reminiscent of the interviewing done by journalists and ethnographers. We also see clients collect data such as by charting their moods over time, and in response to various events that occur. This work mirrors that of quantitative researchers when surveying / sampling. . Next, during or after data collection, there is a ‘script analysis’ of (i) the client’s mental stories and beliefs and (ii) of the behavioral / emotional outcomes that result from these scripts. This analysis may take all sorts of forms such as motivational interviewing, transactional analysis, unblending in Internal Family Systems, the ABCD’s of cognitive behavioral therapy, etc. Once we identify erroneous or self-sabotaging beliefs, they can be released and replaced with more accurate beliefs that allow us to more successfully meet our needs. We can then use this new worldview to write a better next chapter in our life’s story. . Point 3: Use the psychological optimization process to document and revise both the narrative/qualitative stories AND the graphical/statistical/quantitative stories that make up our worldview. . Till next time Young Tim—as with the PhD, so with your life: “may the writing go well!” . This was a key fact being played upon in the award winning HBO show Westworld to endow the artificially intelligent ‘hosts’ with human level self-awareness. &#8617; . |",
            "url": "https://timothyb0912.github.io/blog/philosophy/psychology/2021/11/17/Story-importance.html",
            "relUrl": "/philosophy/psychology/2021/11/17/Story-importance.html",
            "date": " • Nov 17, 2021"
        }
        
    
  
    
        ,"post6": {
            "title": "Questions for Introspection",
            "content": "Questions for Introspection . Dear Young Tim, . Happy Halloween! . I know this month has been more cruel trick than pleasant treat. . Of course, in such circumstances, pay attention to your problems. However, maintain self-control. Hold on (to the handlebars of your life). Work when you’re excited or when you need to work. Sleep when you are weary, and do not let your pride ruin your peace. Sleep consistently. . Once we’re well rested, we’ll want to ask ourselves how we got into our problem situations and how we will resolve them. . To help, this note follows up on last month’s causal graph for introspection. Here’s the graph once more, for memory. . Today’s post lists some questions that may be useful for directing our thinking about each node. The question structure tracks the “user-story” format (GIVEN-WHEN-THEN), which we use to organize the nodes in the introspection graph. Since last month’s entry explained the rationale behind each node and behind the user story format in general, let’s get straight to the questions! . Note, here, I only question the “THEN” and “WHEN” portions of the graph. I’ll supply questions for the “GIVEN” portion of the graph in the future. May these questions be of use to you and to others, and until next dispatch— good luck! . Post-decision feelings (i.e. “Then”) . Description . “What problem am I experiencing?” | “What are all of my feelings right now, emotionally?” (See here, here, and here for diagrams and lists for inspiration.) | “What are all of my feelings right now, physically?” | “What physical behaviors am I displaying?” | “What happened just before I started feeling this way?” | “What was I thinking or saying to myself just before feeling this way?” | . Labelling, chunking, analysis . “What (emotional/cognitive) parts of us are active right now? What parts of our body feel this activation?” | “For each emotional/cognitive part of us that is activated, i.e. experiencing intense physical feelings:” “What is that part’s earliest memory? / What’s the earliest memory where you felt this way or were in this or similar situations?” | “How old were you at the time of this earliest memory?” | “What does this part want to do?” | “What does this part believe, think, or say?” | “What is the positive intent of this part and its feelings?” | “What does this part need?” | “What role does this part and its feelings play in our internal family system?” | “What is this part’s name (if there is a name for it)?” | . | “For each part of us that is activated and desiring behaviors that we see as globally unhelpful, what other parts are they protecting?” | “Are we experiencing any feelings / parts that are polarized or wanting opposing things?” | “What parts of us are stopping us from ‘socially undesirable’ acts?” | “What parts of us are encouraging us to engage in ‘socially promoted acts’?” | “What connects the activating event for this moment’s undesired feelings with the origin story for each active exile (i.e. activated parts of us with emotional wounds, especially from childhood)?” | . Immediate response . “What do I want to feel right now, emotionally?” | “What do I want to feel right now, physically?” | “How can we give the emotionally wounded and activated parts of us some of what they need right now?” | “How can I activate the parts of me with the emotional and behavioral traits I wish to display/use in this situation?” | “What are the next-best emotional states that we can transition to, for example, e.g. fear -&gt; excitement / useful anger?” | “How can I immediately activate appropriately good feelings?” (For example, appreciating a robust challenge.) | . Events causing feelings (i.e. “When”) . Definitions . “What decisions are we making that are leading to our consequences/outcomes?” | “What circumstances / external factors are leading to our consequences?” | “What are our primary consequences or outcomes?” “I.e., what directly results from our decisions?” | “What are our secondary consequences or outcomes?” “I.e., what do our primary consequences lead to?” | . Data-collection . “When do our primary consequences result or materialize, especially in relation to our decisions or circumstances?” | “When do our secondary consequences result or materialize, especially in relation to our primary consequences, decisions, or circumstances?” | “What happens externally (if anything), just before our primary consequences materialize?” | “What happens externally (if anything), just before our secondary consequences materialize?” | “What do we think or feel (if anything), just before our primary consequences materialize?” | “What do we think or feel (if anything), just before our secondary consequences materialize?” | “Where do our primary and secondary consequences occur, metaphorically, procedurally, and/or geographically?” | “How severe are each of the primary and secondary consequences that occur?” | . Hypothesis generation . Past . “Why (briefly, perhaps in the form of a y-statement) did we make the decisions we made?” | “Why do we believe the circumstances that we faced occurred, especially interpersonally?” | If the situation involves others: “What might the other people’s feelings be, and what are all the ways that we can validate these predictions?” | “What might the other people’s beliefs be, and what are all the ways that we can validate these predictions?” | . | . Future . “What primary or secondary consequences do we wish we were experiencing?” “What feelings do we hope these alternative consequences will lead to?” | “What needs do we hope to meet by achieving these alternative consequences?” | “What goals can most swiftly make these alternative consequences a reality?” | “What immediate tasks are necessary to achieve those goals?” | . | “What alternative decisions could we make in the future?” Brainstorm MANY alternatives. | “What alternative circumstances could occur in the future?” | . Counterfactual . “What counterfactual primary outcomes and secondary outcomes could the alternative circumstances produce, holding our decisions constant?” | “What counterfactual primary outcomes and secondary outcomes could our alternative decisions produce, holding our circumstances constant?” | “Do any combinations of alternative decisions AND alternative circumstances, lead to desirable, predicted counterfactual primary and secondary outcomes?” | . Interpersonal . “Who can help us achieve the primary and secondary outcomes we desire?” | “Who is in a similar situation that I could reasonably offer help to?” | “Who is in a similar situation that I could learn from and collaborate with?” | “Who has the most similar story to us in terms of progressing from a similar current state to any better state? What can I learn from them?” | “Who has the most similar story to us in terms of progressing from a similar current state to a worse state? What from them can I learn to avoid?” | “Who has a life trajectory that resulted in primary and secondary outcomes that are closest to those I desire? What can I learn from them?” | .",
            "url": "https://timothyb0912.github.io/blog/psychology/practice/2021/10/31/Questions-for-introspection.html",
            "relUrl": "/psychology/practice/2021/10/31/Questions-for-introspection.html",
            "date": " • Oct 31, 2021"
        }
        
    
  
    
        ,"post7": {
            "title": "An introspection graph",
            "content": "An introspection graph . Context . Dear Young Tim, . This post follows the recent stream of psychology focused dispatches. The context is the familiar one: we face problems in life where there are discrepancies between feelings we have and feelings we want and between outcomes we have and outcomes we want. To achieve the feelings and outcomes we desire, it is helpful to understand how we came to our undesired states in the first place. Moreover, I believe we increase our probability of successfully achieving our goals by systematically investigating the factors currently causing our issues. . In particular, I think it’s helpful to have a set of questions to start from when trying to understand our personal problems. Of course, having a collection of questions is only the first step. Humans have great difficulty holding numerous items in our mind at once, so we’re going to want to chunk these questions to facilitate understanding. Here, I think it is most natural to organize the questions around the process that leads to our undesired state. . Takeaway . The graph below shows our current conceptualization of how we come to undesired states, in general. This graph attempts to explicitly delineate an ‘almost-complete’ set of causal factors that lead to outcomes and feelings that we do not want. Beyond serving as a tool to convey our current understanding, this graph can be a map to guide our process of introspection. . Relation to previous posts . The introspection graph above extends the causal graph for debugging problems that I drew in February: Specifically, the current introspection graph expands the focus of our problem-solving from ‘objective’ outcomes to include our subjective, post-decision feelings. The current graph also expands the ‘constitutional state’ or ‘root cause’ node to expose the interconnected set of variables that affect our decisions and post-decision feelings: . beliefs | goals | failure-states | needs | emotional-wounds | pre-decision feelings | . With this greater specificity, we gain increased insight on where we can intervene to resolve our problems. . Additionally, today’s post relates to May’s note on psychological-optimization. Specifically, the psychological optimization process, as I conceive of it so far, involves three steps: education, introspection, and integration. Today’s post overviews what I mean by introspection. . Introspection elements . To begin, I’ll explain why the introspection graph has its given structure. In particular, many sources have shaped my understanding of how we come to have the feelings and outcomes that we experience. . First, there are the ideas of Nonviolent Communication. Here, the basic premise is that many conflicts arise because of unspecified feelings and needs. Though Nonviolent Communication focuses on interpersonal conflicts, I’ve already described how thinking is internal communication between parts of oneself. Accordingly, the focus on feelings and needs for the resolution of conflicts appears likely to be useful for resolving internal conflicts as well. This motivates the expansion of the causal debugging graph to explicitly denote one’s feelings post-decision and post-outcome. . Second, the role of emotions in driving our decisions is well-known and mercilessly exploited in business applications such as marketing. As a result, we explicitly note our pre-decision feelings and emotions on the introspection graph. . Third, following the learnings of “rational emotive behavior therapy” and “cognitive behavioral therapy,” we note that our beliefs cause our feelings. Here, we draw a distinction between our beliefs about our pre-decision state, leading to our feelings before we make some decision, and our beliefs about the meaning or significance of potential outcomes. These counterfactual beliefs are of the form: if outcome X occurs, then Y. Counterfactual beliefs cause our post-decision feelings, in combination with the actual outcome of the decision and any new, ‘objective’ state we transition into because of our decision’s outcome. Accordingly, we include nodes for both our beliefs about our original state and our counterfactual beliefs, and we include nodes to denote the direct outcome of our decision as well as any secondary consequences of those outcomes. . Fourth, we have two complementary nodes of goals and failure-states. From choice modelling research at one interface of psychology and discrete choice, i.e. within the realm of mathematical psychology, we are reminded that humans (meaning us!) make decisions in pursuit of goals they wish to achieve. Further, positive psychology research reminds us that people have not only goals they wish to actively reach / experience but also undesired or unfavorable failure-states they hope to avoid. These two types of guideposts directly influence what we believe about the world and what we believe about the impact of potential outcomes, hence their placement in the introspection graph. . Finally, we have our underlying needs and emotional wounds. All humans have physical and emotional needs that we attempt to meet. For a non-exhaustive list of such needs (as examples), see here or here. Our goals and failure states are often directly aimed at meeting such needs. Moreover, the degree to which we meet our needs greatly impacts our beliefs about our world. . Likewise, the emotional wounds that we have suffered affect what goals we attempt to achieve and what failure-states we attempt to avoid, all to prevent further hurt. Our wounds also act as a filter through which we interpret the events of life when forming our beliefs. For a non-exhaustive list of such emotional wounds, see here and here. . Suggested use . As stated earlier, I suggest we use the introspection graph to guide our introspection–our asking questions about ourselves. The way I suggest we do this is by answering questions about each node in the introspection graph. This will draw out the story of how our current life’s system generates our undesired feelings and outcomes. . These ‘natural language’ (as opposed to programming language) descriptions are called ‘user-stories’ in software-engineering. The idea is that one’s software system will support or enable these stories. For verification, the stories are often translated into the following template: “GIVEN &lt;some context&gt;, WHEN &lt;something happens&gt;, THEN &lt;something results&gt;.” The introspection graph is color coded to match this format. . As with software engineering, if we don’t like the system’s outputs, we’ll go in and change the system to produce the results we actually want. This presumes we understand the system and its various components. To gain such understanding, we have freedom in how we traverse the graph. . We can proceed one node at a time, asking all questions related to a particular node. We can take repeated tours around the graph, asking one or a few questions at each node, gaining a deeper understanding on each tour. Or, we can use some combination of the two approaches. For now, realize that the choice is ours. Future posts will share some questions that we can ask for each node. . Meanwhile, my default suggestion is to perform repeated tours. It can take a while to ask all questions of interest, so we will likely need more than one questioning session. By performing full tours in each session, we give ourselves some ability to make ‘globally’ informed changes after each questioning session. Additionally, our understanding of a problem often changes in the course of analyzing the problem, and multiple tours expose this in the differing answers to questions within the same node, as seen across tours. . On our questioning tours, I suggest we start by asking questions to clarify our problematic feelings, then we trace the graph “backwards” from the post-decision feelings to our needs and emotional wounds as the last visited node. This follows the order described in February’s debugging post. First we address the immediate problem (i.e. feelings and outcomes), then we address the way we make decisions (i.e. our procedures), and finally we address our constitutional state (e.g. our beliefs, needs, etc.). . Further extensions . To end, I hope we keep in mind that this introspection graph is not ‘complete.’ Some immediately missing elements are nodes to represent our society &amp; culture. Socialization is the internalizing of society’s behavioral norms (i.e. decision-making patterns) and ideologies (i.e. beliefs). If we remain ignorant about how society and our culture affects us, then we remain handicapped in trying to change our resulting decisions and beliefs. . Additionally, I drew this introspection graph from the perspective of intrapersonal problems. I plan to draw a similar graph to represent interpersonal problems, i.e. problems between two or more people. All this to come in future posts! . Until next time Young Tim: May this introspection graph be of use in understanding and thinking through your problems! .",
            "url": "https://timothyb0912.github.io/blog/psychology/2021/09/30/An-introspection-graph.html",
            "relUrl": "/psychology/2021/09/30/An-introspection-graph.html",
            "date": " • Sep 30, 2021"
        }
        
    
  
    
        ,"post8": {
            "title": "Question everything: effective thinking is effective communication.",
            "content": "Question everything: effective thinking is effective communication. . Dear Young Tim, . Peter K. Gerlach—social worker and therapist, is the first person to have notified us of the idea that effective thinking is a form of effective communication. In particular, effective thinking is effective communication amongst differing sub-parts or subselves within us. I know you’ve experienced the feeling of a part of you wanting to stay up and continue on with your research, and another part of you wanting to go to bed because it knows this will be best for your long term health. I know you know the conflict that arises when these parts of you don’t agree. . As R.Kelly said: my mind is telling me NO, but my body—my body is telling me YES. This is yet another instance of two opposing parts of one’s self. . Gerlach’s thesis represents an opportunity for us to improve the way we facilitate dialogue between our conflicting inner parts. This relates to my recent post on coded communications. It is an opportunity for us to move from a commanding relationship, whereby one part of ourselves controls or shuts down another part, to a mutual coaching relationship whereby each part supports the other in what they need or want to do. Put another way, it is moving from a, first-you-then-me relationship, or vice versa, (in the best scenario) to a harmonizing relationship where multiple parts can sing together at once, yet sing together well. . This framing represents another way to conceptualize a major goal of therapeutic healing or coaching. We wish to facilitate harmony between our differing sub-parts and each other, as well as with our actions. . If we look closer, we see that the link between effective thinking and therapy/coaching is multifaceted. I’ve already noted that there is a common goal: harmony of our inner parts in fulfilling our needs. . Beyond common goals, there are also common mechanisms and tools. Mechanistically, effective thinking directly seeks to produce thoughts that are effective in meeting a person’s needs. Similarly, because thoughts cause feelings and actions, therapy/coaching often directly seeks to change our thoughts, so we can evoke sustained change in our feelings and actions. . Instrumentally, therapists and coaches (like good teachers) supply one with suggestions and, more importantly, with questions that help you discover a good path forward for yourself (as with the Socratic method). Another paradigm that makes extensive use of questioning through the Socratic method is critical thinking. As described by Wikipedia, critical thinking “entails effective communication and problem-solving abilities” (e.g. meeting one’s needs). . Synthesizing ideas, there is a clear direction for bettering ourself and putting us back together when we are broken. We can learn to ask ourselves questions to change our thoughts and to facilitate a dialogue between our conflicting parts so we meet all our needs. . Note that such a question-asking agenda is related to cognitive behavioral therapy (CBT). With CBT, occurrences of our thoughts/feelings/actions that are undesirable are to be met with a series of questions. For instance, . what is the activating event that led to this thought/feeling/action? | what specific beliefs do I hold related to this thought/feeling/action? | what are the physical and emotional consequences of these beliefs? | what are various ways of disputing these beliefs with evidence? what further questions are to be asked to make sure my thinking is sound and thorough? | . Indeed, whole books have been written about the link between the Socratic method and therapy. . The point is that such questioning helps change and clarify our thinking. The socratic method in general takes us a step further than our previous post. We want to question not just the process of psychological optimization but all our thoughts, including what we desire in the first place: i.e., the objective function for our psychological optimization process. We want to use such questioning to promote better dialogue and understanding between otherwise conflicting parts, and eventually, in our conflicts with other individuals. The hope is that better thinking and understanding of ourself will translate into our being better able to connect with others, by being better being able question them to promote a similarly deep level of understanding between each other as between our inner parts. . Until next time, good luck and take care! .",
            "url": "https://timothyb0912.github.io/blog/psychology/2021/08/16/Effective-thinking-is-effective-communication.html",
            "relUrl": "/psychology/2021/08/16/Effective-thinking-is-effective-communication.html",
            "date": " • Aug 16, 2021"
        }
        
    
  
    
        ,"post9": {
            "title": "Question the Process",
            "content": "Question the Process . Dear Young Tim, . These posts in 2021, especially these last few, have been about the big picture. They’ve been about the strategic goals that might take you toward a ‘good’ and ‘well-balanced life,’ in some sense. However, the devil is (of course) in the details. It is as if I told you to go on a long trip, and I only told you the countries to go to, with no details of where to go in each country or how to move about within each country. . This post will be different. It will be tactical rather than strategic in nature. It will focus on the questions you should ask of the psychological optimization process in order to achieve your desired outcomes. It’s about a second level of questioning about how to do what you would like. . We will start off with questions of what. What should we do? In May’s post, I spoke of a three-step process of: education, introspection, and then integration. I spoke of applying this process in three areas: traumas, general emotional dysregulations, and underlying executive capacity. However, none of these instructions actually tell you precisely what to do. . In order to determine the particular action items for yourself, you should make use of the causal-debugging post from February. The causal debugging process will furnish you with precise action items for: . ameliorating the immediate undesired outcome | changing your procedures so that the undesired outcome is less likely in the future | changing yourself so that your current and similar problems are less likely or eliminated from your future. | . To complete this causal debugging process, and to connect to the psychological optimization process, you should: educate yourself on what others have done to resolve your current and similar issues, introspect–particularly on how your actions and overall state have contributed to this outcome, and integrate these learnings into planned actions for resolving the problem now and avoiding your problems in the future. . Next, you’ll want to know not just what should you do, but you’ll want to increase the probability of completing these actions. After all, what good is a plan if you don’t execute it? One way to increase the probability of following through on your action items, is to be clear and know exactly why you are taking each action. This brings us to the second set of questions. Why am I performing each action or set of actions? Write down the answers, for safe keeping. Memorize them, so they seep into your unconscious and influences all of your decisions. . Now, in answering why you are deciding to take your specific actions, you should base your answers in a thorough understanding of ‘how.’ How do all of these undesired outcomes occur, and how will my planned interventions achieve a different outcome? You need to have a clear understanding of how you come to the undesired outcomes. Backfill your records, journal, and keep track going forward, throughout the days, over the weeks and months: what is the sequence of events that leads me to behave the way I do and have the thoughts I have? Then, from data, you can generalize to a more abstract, graphical understanding of your problem, that may be easier to work with. In other words, to answer how you do the things you do and how you can change it, we want data, and we want self-awareness over time. . Finally, we want to ask, not just how do I do what I do, but also when are we most likely to engage in behaviors that we want and behaviors that we don’t want. This is not just descriptive statistics and raw data anymore. This is now a model that predicts what is the probability of you engaging in particular behaviors, given particular conditions. So this is the last piece. . At this stage, you’ve asked not only what, but why, and you’ve paid attention to how. Finally you have begun answering when you’ll engage in particular behaviors. Such answers (i.e. such models) provide the necessary material for statistically assessing our therapeutic interventions. Practically, we will want to reduce the frequency with which we end up in conditions that have a high probability of us behaving in undesired ways. Or vice versa. We want to maximize the frequency with which we end up in conditions that have a high probability of us behaving in desired ways. Stepping all the way back, we could also say that we simply want to minimize the total amount of times we behave in undesired ways and maximize the amount of times we behave in desired ways. We can leave discussion of the precise mechanisms to our actual action items. . Overall, if we answer both the qualitative what/why/how questions and the quantitative how and when questions, then we may be able to able to reach all of our strategic goals. In this mode of operation, progress will come via the tactical actions we brainstorm via questioning, AND we’ll hopefully be able to assess the extent that our results are real / distinguishable from chance. With that, good luck till next time! .",
            "url": "https://timothyb0912.github.io/blog/psychology/2021/07/17/Question-the-Process.html",
            "relUrl": "/psychology/2021/07/17/Question-the-Process.html",
            "date": " • Jul 17, 2021"
        }
        
    
  
    
        ,"post10": {
            "title": "Coded Communication",
            "content": "Coded Communication . AKA, what is the difference between: . coaching | counseling | consulting | commanding? | . Dear Young Tim, . If you have not already noticed, there are a range of possible communicative exchanges, in terms of the type of power dynamics that can be displayed. This is especially true when help is being offered from one party to another. Overall, you can imagine at least the following five types of communicative exchanges: . empathetic listening | coaching | counseling | consulting/advising | commanding | . These five communication types lie on a continuum of increasing expectation that the individual who is receiving help will adhere to the suggestion or direction of the person giving the help. This shows up in the following means of relating between the two individuals. . . With empathetic and active listening, you have two parties interacting as equals. One side listens to the other, then the other side listens to the first. This is possibly the most empowering relationship possible. The offered help is immediate in the form of a listening ear, but help is limited to the listening ear as well. . Then there is coaching. Coaching is an enabling relationship that involves empathetic listening, but it goes further by offering suggestions and questions to support the client’s goals. . Next, we have counseling. Here, the individual giving the help takes on more of a leading role than the coach. The counselor points or guides the person receiving help to actions the counselor believes they should take, as opposed to suggesting actions that may help (as a coach would). . Following this are consultants. At their extreme, they are hired as experts, they then hear and learn of the situation at hand, and finally, they deliver a prescription, a report of recommended actions, and then they leave. With a counselor you can imagine some back and forth in the relationship. This is not necessarily expected with a consultant. If desired, the consultant can be hired again for ongoing services. . Straddling the fence between consultants and counselors are therapists. When acting clinically, such as when prescribing medication or therapeutic interventions, the therapist is acting as a consultant. When guiding the client towards behavior change in a back-and-forth manner, the therapist acts more as a counselor. In this sense, a therapist wears both hats of a counselor and consultant. In either case, the therapist relates to the client ‘from above,’ whereas the coach relates to the client ‘from below.’ This is one difference implied in my note from May. . Finally, there are commanding relationships. Here, there is no back and forth at all. The person giving the help tells the person needing help what to do. This is akin to delegation–at best, and at worst, it resembles an authoritarian, parent-child relationship. . Young Tim, what I wish I told you earlier is that communication styles directly correspond to management styles. This would have been a great thing to look for when going to new teams and jobs. When evaluating new managers, does the manager relate to their reports more as a coach or commanders? Or a counselor, or consultant… When does the manager engage in each type of behavior? . Historically, Stitch Fix has been structured such that managers act as coaches more than commanders. And I’ve found that to be great! I wish I knew about such distinctions earlier. . Moreover, Young Tim, you should be aware of these distinctions when you’re helping others. Do you act more as a coach, counselor, consultant, or commander? Which is most appropriate, and when? Be aware of such things, both from others to yourself, and from yourself to others. In all such cases though, be sure to start first with empathetic listening. . Take care, and good luck listening, coaching, and commanding when you must. .",
            "url": "https://timothyb0912.github.io/blog/psychology/2021/06/30/Coded-Communication.html",
            "relUrl": "/psychology/2021/06/30/Coded-Communication.html",
            "date": " • Jun 30, 2021"
        }
        
    
  
    
        ,"post11": {
            "title": "Psychological Optimization",
            "content": "Psychological optimization: On therapeutic healing and coaching . Dear Young Tim, . Despite my regular dispatch of messages, I doubt that I will be successful in sparing you from all calamity. Accordingly, this message is about what to do when disaster strikes. It’s about how to pick yourself up and how to put yourself back together. . I call this process ‘Psychological Optimization.’ I.e., a hill climbing effort to improve your psychological functioning. . . One might say such bettering is what goes on in therapy. This post IS about therapeutic healing. One could also say such bettering is what goes on with coaching. This post IS about coaching too, though there are differences between coaching and therapy: differences which we’ll save to discuss at a later time. . This post is about the similarities and, more specifically, about the general process–the overall process, in fact. . The motivating situation is one where you feel yourself in desperate straights. Something bad has gone on, and now, your decision making capabilities have been compromised. It’s as if you are fighting yourself and life for control of the decision making throne, and you are losing. . . So, to reverse this state of affairs and reach new heights, the overall process is to follow these three steps: . first, educate us about the problem and about what others have found out about it, | second, look within yourself–introspect– and learn about this problem in your life, in particular. | three, integrate all of your learnings from your external and internal worlds and use them to repair and rehabilitate your executive function— your ability to function in general. | . In general, this process maps closely to other decision making frameworks that are well-known and well-regarded. If you think of Gary Klein’s recognition-primed decision making framework, this matches closely to it. . You first develop expertise (that is, you educate yourself), then you make an informed judgement about this particular situation (which involves judging yourself, i.e. introspecting). Finally, you take action (which involves integrating your learnings/judgements). . The psychological optimization process is also related to John Boyd’s OODA loop. In the OODA loop, you first observe and then you orient. In our process, we observe and orient twice. We observe and orient towards the external world–that is, we educate ourselves. Then, we observe and orient the internal world–that is, we introspect. . Finally, we make a decision and we act. That is, we integrate the learnings into our lives. . Now, in order to pick yourself back up, there will likely be multiple areas we need to perform psychological optimization in. At minimum, we’ll need to address the following three areas. . dealing with specific traumatic events | dealing with general emotional dysregulation | dealing with underlying executive functioning issues | . In each of these three areas, we’ll need to repeat the process of: educating ourselves, introspecting, reviewing what we’ve learned, and then integrating our learnings into our lives. We’re going to do that process for each area, and then we’ll likely be in a far better state than when we started. . . We’ll likely be in a far better state. . I hope that helps! For more details, see this presentation of mine. It contains more direction one can cite, and the appendix contains references, from which I took knowledge and inspiration. . Take care, and good luck. .",
            "url": "https://timothyb0912.github.io/blog/psychology/2021/05/30/Psychological-optimization.html",
            "relUrl": "/psychology/2021/05/30/Psychological-optimization.html",
            "date": " • May 30, 2021"
        }
        
    
  
    
        ,"post12": {
            "title": "Journal more and journal systematically",
            "content": "Journal more and journal systematically . Dear Young Tim, . Thanks so much for your patience as I get these notes to you, albeit with delay. The last many months have been trying, to say the least. . Problems . During this time of adversity, I’m reminded that much of adulthood is about trying to rekindle and maintain the enjoyable from your childhood, while still fulfilling your present responsibilities. Journaling—writing one’s subjective thoughts and feelings— is one such enjoyable practice from my youth that still pays dividends. . The most general summary is that life is full of hard times. This includes hardships in one’s personal, professional, familial, social, and romantic life. Considering worldwide as opposed to individualistic concerns, our hardships also include the global Covid-19 pandemic and economic recession that are still raging as well. None of this even begins to address the fact that tons of people around the world and in the United States live harsher lives than myself. Under such conditions, one may experience many distressed thoughts. Indeed, without a management system, one may find oneself drowning in a deluge of thoughts and emotions. . (Partial) Solutions . In such contexts, journaling has long been one way for me to capture my steady stream of ideas and inner monologues. For instance, in high school, I used my running logs to record not just what workouts I ran but also whatever I thought about while running. In graduate school, I turned to using a research journal that housed my thoughts and feelings on potential research topics. After school, I’ve returned to journaling, now about the full range of concerns in my life, from research to home repairs and everything in between. . Young Tim, this third wave of journaling is different from the first two. It’s more all-encompassing. All topics are fair game for journaling. It’s more systematic. I use self-determined triggers for some journaling activities. I use templates and prompts for some journaling activities. I organize my journals, by more than just chronological order. While these practices certainly entail additional effort, the benefits have been worth far more than the costs. . For instance, the immediate benefits I’ve enjoyed include . greater self-respect as I acknowledge, fully engage with, and record my thoughts and feelings for all topics as opposed to only a select few. | deeper thinking, more regularly, as I complete pre-planned templates (e.g. for decision records). These templates inspire further thought as they guide me to answer questions I otherwise may forget / neglect to ask. | better guidance of my thoughts via broadening questions, especially when emotionally flustered. E.g., I took a conversations course that posited that the one who most guides a conversation is the one asking the questions, not the one doing the answering and making statements. With question prompts coming from cognitive behavioral therapy, I’ve been frequently able to direct my thinking to more logical, less catastrophized, and less distorted ends, rather than being merely subject to them. | increased comfort from always being able to talk with oneself on the page, especially when one cannot talk with others in the moment. | . Implementation . Procedures . So how have I garnered these benefits? At a high level, I adopted the following system. . First, I take short, on-the-fly notes about my thoughts, in real-time. These are the thoughts that arise in a ‘disrupting’ manner— thoughts that are not focused the specific task you are doing. Such on-the-fly notes are typically (though not always) unstructured. . Secondly, I use structured notes of sundry kinds. These notes are typically of short to medium length. That is, a few sentences to a few paragraphs. These might be the notes that correspond to ‘disputing a belief with evidence’ in cognitive behavioral therapy, or the decision templates utilized in architecture decision records. . Lastly, I use unstructured notes to capture medium to long reflections and thoughts. These notes are typically answers to single question prompts, or they are complex, spontaneous thoughts, born of long-running hyperfocus on a particular topic. Examples here may be one-time questions that ask me to recount particular meaningful events from my past / childhood. . Between these three types of notes, I have been/felt able to capture all of the thoughts I desire. . Tooling . Okay, let’s get beyond the high level description. How do I actually do this? . Fundamentally, I journal digitally using git repositories to store the three types of journaling notes. I store the on-the-fly notes in json files (i.e., ‘issues’) using git-bug. I store the structured and unstructured notes in markdown in the repositories using git as well. Software such as nb and kb are additionally useful for storing and managing the markdown notes in the repository. They provide searching, linking, and tagging functionality. . Beyond the software tools I use to facilitate journaling, I also draw from external information resources as well. For instance, I often fill in structured journaling notes by answering cognitive behavioral therapy questions for identifying emotional and behavioral consequences of one’s beliefs. Similarly, I use templates from resources such as y-statements and architecture decision records to journal my thoughts about decisions that I have to make. Lastly, I make liberal use of writing, journaling, and reflecting prompts that I find online. These prompts often guide my unstructured journaling notes. . Conclusion . Young Tim, if you engage in such journaling practices consistently, over a long period of time, then in addition to the immediate benefits described above, you should also gain longer-term benefits. Your anxiety should decrease as you live a life where you capture rather than lose most thoughts, and your anxiety no longer needs to keep track of important facts, assessments, and deadlines. You should also have joys and pleasant surprises from looking back at your past thoughts, being delighted as interesting ideas reappear over time and form new connections as you live through new experiences. Lastly, you should materially benefit from trend spotting and insight / hypothesis generation. With your thoughts preserved for future review, you should increasingly be able to spot trends in your thinking and experience: patterns which you can leverage or change to achieve the state of life that you desire. . So without further ado, please, go forth and journal–a lot! .",
            "url": "https://timothyb0912.github.io/blog/practice/2021/04/30/Journal-systematically.html",
            "relUrl": "/practice/2021/04/30/Journal-systematically.html",
            "date": " • Apr 30, 2021"
        }
        
    
  
    
        ,"post13": {
            "title": "More choices, faster",
            "content": "Make more choices, and make them faster . Context . Every man builds his world in his own image. He has the power to choose, but no power to escape the necessity of choice. If he abdicates his power, he abdicates the status of man, and the grinding of the irrational is what he achieves as his sphere of existence - by his own choice. –Ayn Rand, Atlas Shrugged . Dear Young Tim, I have heard it said that a good life is the sum of many good decisions, mixed with favorable circumstances. And indeed, in Principles, Ray Dalio said “the quality of our lives depends on the quality of the decisions we make” (p. 15). . Young Tim, given that you study and model how humans make choices, this should be encouraging news. It means that, if you can study and learn from how other people make choices, and if you study and learn and control how you make choices, then you too can make good choices and live a good life! . Of course, the difficulty lies in executing such a plan. It would be wonderful if I could live a peaceful life without adversity, solely being left to my decision making efforts to have fun and build good health. Unfortunately, life doesn’t work this way for myself or for most people. . Problem . In reality, we face many problems. Life first gives us the test, then asks us to make multiple choices, and only gives us the lesson afterwards. These problems that life hands us demand that we make choices to escape or avoid their negative effects. Moreover, these problems have timeframes that dictate when it is “too late” to make decisions that get you what you desire in life. . For example, if you wish to stay in the apartment that you currently rent, but you lose your job and cannot pay rent, then there is a certain time after which you will have missed so many months of rent that it will be deemed “too late” and you will be evicted. . The reality of these time frames can be terrifying, especially when the problems arise suddenly and outside of our control. Such terror can slow down or cripple our decision making, at exactly the moments that making timely choices is of the utmost importance. This is where you get analysis paralysis and freezing or fleeing from the decision. . For instance, when I was ten, my grandmother fell down the stairs in my family’s home and suffered a stroke. My mother came home to find her elderly mother crumpled at the base of the stairs with blood pooled around her head. Understandably, my mother panicked, and she froze for many minutes. She was unable to make any decision, thereby costing precious time that emergency healthcare workers could have been using to come bring care to my grandmother. Hopefully you never have to face such terrible decisions, but the fact remains that not deciding in a timely fashion can further exacerbate existing problems and cause new ones. . If we don’t make decisions at the speed of relevance, then we may experience opportunity costs from not making good decisions quickly, procrastination-induced-suffering / stress as you belabor decisions, and “times-out” costs from not choosing / choosing the status-quo by default. . Solution . Strategy . The solution to these issues is to make more high-quality / thoughtful decisions, and to make these decisions faster. . One direct strategy is to stop freezing when decisions must be made. Of course, taking on all human evolutionary response is easier said than done. . One thing we can do is to learn from those who are forced to make tons of good decisions in high stress situations when beset by many problems: e.g., firefighters and members of the military. Amongst these individuals, you come across a common saying that: . Under pressure, you don’t rise to the occasion, you sink to the level of your training. . That is, to stop freezing when we must make decisions under time-pressure, we should train ourselves to continuously and quickly make thoughtful and value- aligned decisions, even and especially when not under pressure. Then when we face pressurized decisions, we will hopefully have a higher probability of decision-making success. . Tactics . So, how can we do this? I suggest we proceed with the following three considerations. . First, Jeff Bezos, CEO of Amazon, once said that “if I make 3 good decisions a day, that’s enough.” We should try to behave similarly, at least as a starting point. To be concrete, try to make and record at least the following four choices, in some fashion (whether large or small), every day. Choose: . how you will use your time (primary resource allocation) | how you will partially or totally solve one of your problems (strategy) | how will you execute your problem solving strategies (procedures / tactics) | how you will take action today to implement your problem solving procedures | . Making these four decisions daily will ensure that at minimum, you manage your scarcest non-physical resource, you think about how to design your life such that you solve or avoid problems while achieving your goals, and that you act to improve your life rather than just wishing for improvement. . If you can get into a habit of making these decisions all the time, and making them quickly, then hopefully you will be able to continue such habits when you are in the midst of dealing with more time-sensitive problems. . Second, use templates to record and structure one’s decision making. Having a template can help take some of the guesswork out of deciding by giving us a way to frame and structure our thoughts. And once we record our decisions in such templates, the standardization will better enable us to review the decisions we have made and why. With review, we’ll be able to diagnose and improve our decision making abilities. . As a template for storing decisions about time use, I enjoy using a yaml file with the following structure (where ts stands for timestamp): . 2021-03-31_Wednesday: - start_ts: &quot;21:30&quot; stop_ts: &quot;22:30&quot; title: &quot;Write blog post (full-draft)&quot; completed: true notes: | Hello, world. . To record decisions about how to partially or totally solve problems, I like to use y-statements in conjunction with architecture decision records. Lastly, to record decisions about how to execute a problem solving strategy, I use the following template for how-to guides. See the template links for more information. . Third, proceed with care. While it’s important to go fast, we want to go fast forever. To that extent we should make decisions only as fast as we won’t regret them. . For non-reversible decisions with worst-case outcomes that risk our survival or goal attainment, make these decisions as slowly as needed to be careful and thoughtful, but no slower. For reversible decisions with mild worst-case scenarios, decide swiftly. Just do it! For all other decisions, do moderate analysis, then quickly use experiments and staged rollouts to empirically choose between options. See the speed-reversibility matrix for more information. . If you follow these advice, you may be so fortunate to both learn how to make good decisions and to encounter favorable circumstances along your life’s journey. Good luck! .",
            "url": "https://timothyb0912.github.io/blog/reading/2021/03/31/More-choices-faster.html",
            "relUrl": "/reading/2021/03/31/More-choices-faster.html",
            "date": " • Mar 31, 2021"
        }
        
    
  
    
        ,"post14": {
            "title": "Causal graphs and principles for designing your life's systems",
            "content": "Causal graphs and principles for designing your life’s systems . Dear Young Tim, . In life, you will inevitably design and debug systems. Think of the physical systems you interact with. Your cardiovascular, dental, immune, and transportation systems. Think of the procedural systems you interact with. Your personal research system, your personal finance system, your personal software system, your business operational system. . Trust, time is a short man. If you want to see far, then it is in your best interest to learn from others. To stand on the shoulders of giants. In other words, read! Learn from the experiences of others. . What I wish I told you earlier is that debugging is a three step process. . Takeaways . First, you respond to the immediate negative outcome. . Second, you modify your procedures and decision-making processes so this particular issue does not happen again. . Third, you address the root causes about oneself and about one’s organization so this problem is less likely to recur in the future. . How did I get to these conclusions? Through books, a causal graphical approach, a general systems view and causal graph, and through gleaned principles of debugging. . Right. Let me unpack that… . Books . First, the books. I read Principles by Ray Dalio and Work the System by Sam Carpenter. . Perspective . While reading, I made use of the following key idea from Michael Margolis. . Graphing causal models, while reading econometric papers can make it easy to understand assumptions that are vague in prose and to isolate those assumptions that are crucial to believe the main causal claims [strikethrough by Timothy] . With that perspective, I approached these two books as causal graphical novels, rather than as traditional books. . Graphs . The first graph that stood out to me was the following. . Dalio wrote: . So, the quality of our lives depends on the quality of the decisions we make. We literally make millions of decisions that add up to the consequences that are our lives. (p. 16) . . Next, I saw Carpenter write about a life of linear systems. Specifically, he said . Linear: For our purposes, this is how most systems execute themselves, in a 1-2-3 stepped progression Yes, there are always minor external and internal variables [i.e., circumstance…]. But here in the real world, we are streamlining things so events can be understood and manipulated. (p. 10) . . Putting the two together, I see the sequence of my decisions AS the system from Carpenter or the “machine” from Dalio. . That’s the first milestone. . I am in charge of managing my life, aka the sequence of decisions I make, that, in combination with all the circumstance of life, result in the outcomes that I experience. . Next, what happens when that system produces outcomes that I don’t like? How should I debug? . Dalio writes that, . It is important to distinguish root causes from proximate causes. (Principles, p.31) . . Proximate causes typically are the actions or lack of actions that lead to problems— e.g., “I missed the train because I didn’t check the train schedule.” (Principles, p.31) . Following this, Dalio notes that . Root causes are the deeper reasons behind the proximate cause: “I didn’t check the schedule because I am forgetful”—a root cause. (Principles, p.31) . . Here, we see that debugging or responding to a system that is not producing desirable outcomes has at least two steps. We change how we make our decisions to change our proximate causes. We change how we are to change our root causes. . Not to be left out, Carpenter writes that, . […] a problem is […] a wake-up call. This means that once the manager corrects the immediate negative effects, there is a second step. It is this second step that is key: The problem’s cause is traced to the errant subsystem, which is then modified so that the problem doesn’t happen again. –Work the System . Here we see the emphasis placed on a different section of our graph. . . From Carpenter, we see that when debugging and responding to a system that is not producing desirable outcomes, we must first respond to the immediate negative outcome, and then we need to fix the subsystem (our constitution / makeup and decisions) “so the problem doesn’t happen again.” . . Putting it together, we see that debugging is a three step process. . Address the immediate ramifications of the undesired outcome. | Change the way you make decisions so that we change our proximate causes. | Change the way you are as a person so you change your root causes. | . That Young Tim, is what I wish I told you many years ago. .",
            "url": "https://timothyb0912.github.io/blog/reading/2021/02/28/Causally-Engineering-Life.html",
            "relUrl": "/reading/2021/02/28/Causally-Engineering-Life.html",
            "date": " • Feb 28, 2021"
        }
        
    
  
    
        ,"post15": {
            "title": "Document everything and document first",
            "content": "Document everything and document first . Dear Young Tim, . Happy New Year and welcome to 2021! As with every new year, January has brought a flurry of change. And with change comes growing pains. . Specifically, both at work and at home, I experienced a series of intermittent, system failure incidents. These incidents were caused by both changing circumstances and lack of sufficiently robust preventative measures. Moreover, the failures have been widespread, including everything from home plumbing systems, to time management systems, to software systems. As the old saying goes, “when it rains, it pours.” . When responding to the simultaneous failure of many systems, what I often need the most is to know an algorithm: (1) what to do, and (2) step-by-step instructions of how to do it. Indeed, when the spout in my bathroom is leaking heavily, all I want is to know how to fix it—quickly. . Curiously, the need for knowing what and how to do things exists outside of incident response. For instance, to collaborate well with others, it’s helpful if I am clear about what I will do and how I will do it. Likewise, to successfully delegate a task to someone else, both I and the person doing the task need to be completely clear on what the task entails and how it should be done. . Even when only considering how I work, as opposed to how I work with others, it is helpful to be clear about what I should do and how I do them. In particular, if I want my products and services to be of high quality, by default, then I likely need to standardize my processes around pre-determined methods that yield high quality results. Similarly, if I want to accurately plan my time and projects, standardizing around a set of procedures for what and how to do something allows apples-to-apples comparisons and estimates of how long that and similar tasks will take in the future. And finally, if I want to improve my process for doing something, having a standardized process simplifies matters greatly compared to an erratic process. It’s hard to tell signal from noise when the variance is high! . Young Tim, you will always deal with problematic systems; you will always accomplish more by working with others than alone; and you will always be busy planning and trying to improve yourself. Accordingly, you should meticulously record the procedures for all tasks that are necessary for the correct functioning of any systems that you are responsible for. . Why? Because if you document the algorithms and procedures for doing everything needed to keep your systems working, then you can live with greater peace knowing that when something breaks, you’ll have knowledge to fix it. Additionally, you’ll be more excited about life when you can take an otherwise abstract process that isn’t working well, and visibly transform it into a process that you expect to be better. Finally, if you diligently make use of your documented procedures and if you track your time while doing so, then you’ll be able to make better decisions and plans because you will have more usable information (i.e. standardized / cleaned) and more useful information (i.e. apples-to-apples with less missing data). . Okay if above sounds reasonable and you want some lighthearted reinforcement, or if you want a pictorial, minimal-word TL;DR, see the following presentation: . Lastly, see the following template for writing how-to guides as a starting point for documenting your own procedures and algorithms. . Till next time, .",
            "url": "https://timothyb0912.github.io/blog/reading/2021/01/31/Document-everything.html",
            "relUrl": "/reading/2021/01/31/Document-everything.html",
            "date": " • Jan 31, 2021"
        }
        
    
  
    
        ,"post16": {
            "title": "How I'd do it all over again",
            "content": "How I’d do it all over again . Dear Young Tim, . It’s the end of 2020, and what a tempest this year has been! A friend of ours recently asked, “how would you do the PhD again, if you could?” . I like this question. For one, the topic is timely. After all, when better to reflect than at the end of a year? Second, the topic is well-scoped for me, right now. It makes me consider a usefully long yet recent period in my life. Finally, the topic is widely relevant: I think the question of how to be a PhD student is intimately related to another question I frequently receive— how to be(come) a data scientist. . Without further ado, here (at a strategic rather than tactical level) is my answer. . Focus on “how” . Focus on how you do it. What you do is important, of course. But, how you do it is also important, and you frequently overlook this. Be interested in how to do things effectively. How to do them elegantly, efficiently, equitably. Be even more interested in how to avoid doing things (e.g., by enabling others and automating everything). . Move fast and with endurance . Speed is important. It is important to get (correct) results quickly—to learn quickly. It is important to finish unimportant tasks quickly so you can spend time on what and whom you find important. . However, sustained speed is most important. Having the ability to go fast forever is a key dimension of being in great shape. Here, the goal is to make continual, rapid progress. We want to move forward while backsliding as little as possible. . In particular, we want to ensure that as we build new knowledge that we don’t forget things. Memory tests with spaced repetition, e.g. cue-cards or anki questions, are extremely valuable tools for this. Similarly, as we build new software tools (or as operating conditions change), we want to make sure that we don’t break existing code. So far, I’ve had much success using automated software tests to provide first indications of broken code, especially if the tests run fast and cover my entire codebase. Overall, if we don’t forget things and don’t break things while building up our knowledge and tools, then we stand a chance of sustaining our rapid progress over the long-term. . Beyond minimizing backsliding, you should manage your progress so it remains as you desire, in all aspects of life. Measure yourself in terms of how quickly you complete valuable projects. Give yourself more points for speed and more points for greater derived value. Read papers quickly. Know that you can learn quickly by doing so every day, every week, etc. . Put differently, your ability to quickly produce value in your life, for the long run, is paramount. Track, grow, and maintain it. . Be thoughtful and reflective . Hold, set aside time for, and record retrospectives. Constantly recenter what is important and why you are doing what you are doing. Record what had happened and why you made the decisions you made. Reminisce on the fun and successes you had! Learn from the setbacks and failures you had. Record that you learned new things and what they were. Brainstorm how you could do whatever you did, better. And don’t forget, track your growth. . Be inquisitive . Be in the habit of asking and answering lots of relevant and useful questions. Be complete or at least thorough in the questions you ask: who, what, where, when, and why? Do statistical methods research as science, not mathematics. As you conduct this research, remember the causal foundations of applied probability and statistics. . Be rigorous and reproducible . Be reproducible. Document your entire project workflow, software dependencies, and hardware dependencies. | Work transparently. I.e., be version controlled, be publicly viewable, and be publicly installable. Be open for scrutiny, review and improvement by others. | Research with rigor. Do simulations to investigate your methods. Create and investigate fake data. | . Constantly re-orient yourself . When tramping through uncharted territory, frequently re-orient yourself relative to a known point to ensure progress towards your destination. On a standard cadence, re-orient your attention to important problems in your field. Likewise, on a standard cadence, re-orient your attention to how you do your work. . Read about how to do great research. | Learn about how to learn. | Read and watch people speak about how to produce great software. | Re-run your unit tests after every change to make sure you haven’t broken anything. | Work under and learn from those who are affecting organizational change. | . And lastly, make a habit of reading and speaking to people about what they consider important. Do this regularly. . Create usefully . While a subjective choice, I think it’s best to focus on solving problems faced by many people. To be helpful. Even better, to create tools so people can solve their own problems. This is highly appreciated and impactful. Best yet though, is to build solutions that ensure problems don’t arise in the first place. . Practice continually . If something is worth doing, then do it regularly. For instance, maintain a blog during grad school. Blog frequently, as in every week–driven by daily writings. Again, document your learning. . In short, practice your crafts! Write every day: you are an author. Code every day: you are a software designer, architect, and engineer. Do statistics every day: you are a statistician. Read every day: you are a consumer of research. Plan every day: you are a planner. Learn every day: you are a student. Build every day: you are a creator. . If these don’t sound like you now, that’s fine. Fake it till you make it. After all, you are what you repeatedly do. . Stay healthy . Last, but not least: take care of yourself. You cannot go fast forever if you do not last forever. So sleep well. Eat well. Exercise well. Meditate well. .",
            "url": "https://timothyb0912.github.io/blog/philosophy/2020/12/24/Do-Over.html",
            "relUrl": "/philosophy/2020/12/24/Do-Over.html",
            "date": " • Dec 24, 2020"
        }
        
    
  
    
        ,"post17": {
            "title": "First things first: On protocols and unit tests",
            "content": "First things first: Protocols, type annotations, and unit tests . Motivation . Dear Young Tim, . Strive to be more trustworthy. Strive to be more effective, efficient, and well-prepared for problems. Strive to be great: to others, to your (future) self, and in how you create. These goals complement each other. Together, they help you “be” better, and they point to better ways of being. . Concretely, these goals highlight the primacy of, the need for, and the benefits of automated software testing. Indeed, trusting your code or your results is hard without tests that provide a basic level of claim verification. Being well prepared for coding errors is hard without tests to help pinpoint the issues. And lastly, refactoring or extending your work is unlikely to be a great experience for your future self without tests to confirm your software’s functionality. . The problem . Alright then. Given its importance, you can understand why testing before coding is the right thing to do. You shouldn’t begin a journey without a way of knowing when you’ve reached your destination. Unfortunately, purity aside, testing before coding is hard! . This hardness stems, in part, from uncertainty. Without code, we are uncertain of what we should test. Even with existing code, it can be hard to know where to begin testing. . The solution . To address this testing barrier, we’ll remove some of our uncertainty through analysis, decision-making, and codification. In particular, recall September’s project lifecycle description. . We should first understand the system we want to change. Specifically, we should perform an object analysis of the system. What are or what should be the system’s basic objects and entities? What are their functions and attributes? . Next, make some design decisions: sketch the basics of how the code should function. What functions do you call? What objects do you instantiate? How do those objects interact? We’ll codify these decisions in function signatures and object protocols. . Finally, we’ll test to ensure that we respect our signatures and protocols throughout the codebase. This is our most basic way to make our software trustworthy: ensuring that it does what we say it does. . Description . Of course, the described process will be more helpful if you know what I’m referring to. By function signatures I’m referring to type declarations of your functions’ inputs and outputs. Compare . def calculate_loss(predictions, targets): ... . versus . def calculate_loss( predictions: np.ndarray, targets: np.ndarray ) -&gt; np.ndarray: ... . Immediately, the second definition provides answers to questions. What types of inputs are expected? What types of outputs are expected? . By protocols, I’m referring to objects that serve as placeholders and guidelines for how other objects should look and behave. For illustration, consider the following base class. It defines required attributes and methods for objects that provide or implement the protocol. . from typing import Protocol class Model(Protocol): num_design_cols : int @classmethod def from_params(cls, params: np.ndarray) -&gt; &quot;Model&quot;: pass def predict(self, inputs: np.ndarray) -&gt; np.ndarray: pass def simulate( self, inputs: np.ndarray, num_simulations: int, seed : int=25 ) -&gt; np.ndarray: pass def save(self, output_path: str) -&gt; bool: pass . This Protocol specifies that your model classes should have . a num_design_cols attribute of integer type, | a from_params method that instantiates the class from a numpy array, | a predict method that takes a numpy array of inputs and returns a numpy array of outputs, | a simulate method that requires an integer number of simulations and takes an optional, integer random seed, alongside one’s input and output numpy arrays, | a save method that takes an output path to save one’s model parameters to and returns a boolean indicating success of the process. | . Ideally, the attributes and functionality of one’s objects will be informed by one’s object analysis. How do you want to interact with one’s object’s? Perhaps you wish to do the following. . from my_project import ( load_data, load_params, Model, FINAL_PARAMETER_PATH ) design, targets = load_data() params = load_params() model = Model.from_params(params) predictions = model.predict(design) targets_simulated = model.simulate( design, num_simulations=100, seed=901 ) # Further training and/or analysis ... model.save(FINAL_PARAMETER_PATH) . As written, the protocol above supports such a workflow. . In the best case scenario, protocols will help you prototype faster. They’ll focus your attention on high level design decisions, sans implementation details. Thinking through and solving problems at this stage can save hours and weeks of effort later on. . Additionally, protocols should increase the modularity of your code. By referring to protocols in one’s signatures, you can change any providing object’s implementation choices without affecting any other parts of one’s code, so long as the provider adheres to the protocol. Indeed, protocol adherence enables the abstraction needed to support each provider’s uniqueness in their implementation. Having commonality in method/attribute presence and type signature is what affords other objects/clients the luxury of ignoring the details of how each provider does what it does. . Implementation . Once we’ve defined the type signatures and protocols for our project’s functions and methods, we’re ready to begin testing. Specifically, we should test the basics. We’ll start with testing the “happy path.” Our type signatures declare that given “valid” inputs of specified types (however we define valid), we will get back outputs of specific types. We will test that these statements are true. . For such contract testing (i.e., input-output type checking), the libraries Typeguard and PyContracts should be helpful. I especially recommend PyContracts because of it’s ability to conveniently disable all type checking when not running tests using contracts.disable_all() You can get started after installation by adding the following decorator to functions in your source code with type signatures. . import contracts from contracts import contract @contract def my_func(arg_1: Type1, arg_2: Type2) -&gt; ReturnType: pass contracts.disable_all() . Then, one can include following tests such as the following. . import my_project def test_my_func_signature(): # Define valid function arguments arg_1 = ... arg_2 = ... # Enable signature testing # Comment out the following line if using typeguard my_project.contracts.enable_all() # Exercise the function result = my_project.my_func(arg_1, arg_2) return . Alternately, one can comment out the contracts.enable_all() command in one’s test, keep one’s source file free of decorators, and use pytest --typeguard-package=src/my_project . from one’s project root to run one’s tests. Decorating all of one’s functions is tedious and typeguard provides a useful workaround for such concerns. Either way, such tests will verify that your code satisfies its advertised type signatures, at least under the tested circumstances. . Next, our protocols declare the necessary attributes, methods, and type signatures of our objects. We simply need to test that our instantiated objects actually implement the protocol. This is as simple as creating tests like the following . import my_project from test_fixtures import load_test_inputs # Enable contract testing my_project.contracts.enable_all() def test_protocol_implementation( model_class=my_project.MyFancyModel, model_protocol=my_project.Model, ) -&gt; bool: # Replace with any alternative instantiation process if necessary model = model_class() assert isinstance(model, model_protocol) return True def test_protocol_signatures(model_class=my_project.MyFancyModel,) -&gt; bool: design = load_test_inputs() # Replace with any alternative instantiation process if necessary model = model_class() # Test that we can execute the function under test without error # Unexpected types on input or output we will raise an error thanks to # `contracts.enable_all()` result = model.predict(design) return True . Here, we rely on the fact that isinstance(model, model_protocol) will raise an error if model doesn’t have all of the required attributes and implement all of the methods required by model_protocol. We then again rely on contracts to perform the actual type signature testing of our method inputs and outputs. . Extension . The contract testing procedure just described is only as useful as the types it is testing for. In particular, we will catch more of our own errors as our types become more specific. For instance, we can catch more errors using contracts and . def loss( targets: &quot;array[N], (N&gt;0, (0|1))&quot;, predictions: &quot;array[N], (N&gt;0,&gt;0,&lt;1)&quot;, ) -&gt; &quot;int,&gt;=0&quot;: ... . as compared to using standard and less specific type hints with . from numbers import Number def loss( targets: np.ndarray, predictions: np.ndarray, ) -&gt; Number: ... . The former type signature will check for . binary values in targets, | values in the unit interval within predictions, | unidimensionality and equal shape of both targets and predictions, | and non-negative scalars as outputs. | . I’ve silently violated these conditions in the past, and I have spent far too much time searching for the errors because of these types of issues (all puns intended). No more. Note that the python-vtypes package provides similar and even more general capabilities. python-vtypes has the added ability of letting you define types that serve as drop-in replacements for regular python types, no decorators necessary. Note that this enables typeguard to also check for more specific types than those natively defined in the typing module. . Workflow integration . From the last section, you will have created detailed types. They should encapsulate the important essence of what you expect from your object attributes, function inputs, and function outputs. You will also have taken the advice of the Implementation section, so you will have defined simple tests of your function signatures. You should feel great about this! However, you should go even further. . You should use your initial signature and protocol tests as a starting point for the virtuous cycle of test-driven development. Specifically, you now have tests but no functioning code. By design, you have failing tests. You should now write the most minimal code that passes one of your tests. Note, this code will likely NOT be the code that you want. This is done purposefully. That the incorrect function passes the tests but doesn’t do what we want means that we are missing other tests! We should create a test suite that specifies our function’s full set of necessary conditions. Then we’ll write code to iteratively pass each of those tests. After this process or some number of repetitions of it, we’ll have implemented and fully tested a desired unit of code. . Example . With all of the above in mind, here’s a concrete example. As with previous posts, this is part of an effort to revise my paper . Brathwaite, Timothy. “Check yourself before you wreck yourself: Assessing discrete choice models through predictive simulations.” arXiv preprint arXiv:1806.02307 (2018). . Here was the context. When trying to understand estimated models, I become Picasso. Well, not the same, but I do make pictures. Plots, specifically. Lots of them. . I decided to share this code in a python package, Checkrs. It would be great to be able to automatically test that my functions plot what they say they are plotting. Unfortunately, writing unit tests for matplotlib is a terrible experience. Since Altair can output plots in a standardized Vega-Lite JSON, I figured it would be easy to test. . To get started in porting my matplotlib functions to Altair, I followed the process described above. I started by defining protocols of the basic objects: the chart. I called it a view because I think of each chart as one way to view one’s data. This protocol would therefore be a base object for all charts. . @attr.s class View(Protocol): &quot;&quot;&quot; Base class for Checkrs visualizations. Provides a view of one&#39;s data. &quot;&quot;&quot; theme : PlotTheme @classmethod def from_chart_data(cls, data: ChartData) -&gt; &quot;View&quot;: &quot;&quot;&quot; Instantiates the view from the given `ChartData`. &quot;&quot;&quot; pass def draw(self, backend: str) -&gt; ViewObject: &quot;&quot;&quot; Renders the view of the data using a specified backend. &quot;&quot;&quot; pass def save(self, filename: str) -&gt; bool: &quot;&quot;&quot; Saves the view of the data using the appropriate backend for the filename&#39;s extension. Returns True if saving succeeded. &quot;&quot;&quot; pass . It’s the basics. . I need to instantiate the chart from data, | I need to store the styling options for the plot (theme), | I need to draw the chart using some library (backend), | and I need to save the chart. | . Protocol in hand, I wrote the basic tests. . def test_draw_signature(self): &quot;&quot;&quot; GIVEN a chart instantiated with valid_chart_data WHEN we call the draw method with any valid keyword-argument THEN we receive the appropriate matplotlib.Figure or an altair.Chart &quot;&quot;&quot; for backend, view in product(self.backends, self.charts_all): chart = view.from_chart_data(data=self.data) manipulable_object = chart.draw(backend=backend) self.assertIsInstance(manipulable_object, (ggplot, TopLevelMixin)) def test_save_functionality(self): &quot;&quot;&quot; GIVEN a chart instantiated with valid_chart_data WHEN we call the save method with any valid keyword-argument THEN the appropriate file will be saved to its appropriate location AND we will be returned a boolean indicating saving success &quot;&quot;&quot; if not os.path.isdir(self.temp_dir): os.mkdir(self.temp_dir) # Make a directory to hold the test plots filename = os.path.join(self.temp_dir, &quot;test_filename&quot;) try: for ext, view in product(self.extensions, self.charts_all): chart = view.from_chart_data(data=self.data) full_path_current = filename + ext # Ensure missing file, create the file, ensure existing file self.assertFalse(os.path.exists(full_path_current)) result = chart.save(full_path_current) self.assertIsInstance(result, bool) self.assertTrue(os.path.exists(full_path_current)) finally: # Clear up test plots even if failure happens shutil.rmtree(self.temp_dir, ignore_errors=True) . I’m of course skipping details of these tests, but the basic points are the following. self.charts_all is a list of Views. Then, chart = view.from_chart_data(data=self.data) tests the constructor method. It also tests whether the implementing class has a theme and the methods defined by View. Next, one function tests chart.draw and another function tests chart.save. This fully tests the protocol. Have a look at the following three files for all the details of the protocol construction, implementing chart construction, and tests. . To top it off, I set up Github Actions workflow to perform the tests and display the results automatically upon pushing changes to the package repository. Both the original tests and the continuous testing via Github increase the package’s trustworthiness. So, despite having to learn by fire along the way, it’s a win in the end! . .",
            "url": "https://timothyb0912.github.io/blog/practice/2020/11/17/First-things-first.html",
            "relUrl": "/practice/2020/11/17/First-things-first.html",
            "date": " • Nov 17, 2020"
        }
        
    
  
    
        ,"post18": {
            "title": "On command line interfaces (CLIs)",
            "content": "On the use of command line interfaces . Dear Young Tim, . You will want your analyses and overall projects to be reproducible. Reproducibility will necessitate description. You will need to describe what your analysis does and how to do it. One language to describe your analysis and project will be a directed acyclic graph (DAG). . DAGs are everywhere. “Analysis is a DAG.” Workflows are DAGs. DAGs have nodes, by definition. . These nodes are: . functions (and “functions are things”) | workflow steps (see last month’s post) | workflow tasks | workflow stages | . These things, i.e, these nodes: . take inputs (possibly None), | act in predefined ways, | and return outputs (possibly None). | . Managing our nodes’ inputs and outputs (I/O) will enable our project’s reproducibility. Indeed, we cannot reproduce what we did not record. We must record the inputs that led to specific outputs, to later reproduce those outputs. . Ideally, we should manage our I/O with plain text configuration files (to ease tracking). These configuration files should specify, for each node: . all inputs or input data locations, and | the locations of all outputs to be created. | . Next, we need to ingest these configuration files to use them. A useful ingestion pattern is to write the nodes / functions as command line interfaces (CLIs) that take the configuration as input. See tools such as Hydra for enabling such CLIs. Configuration-driven CLIs increase reproducibility by facilitating use of program call patterns that can be easily tracked for later replay. . Finally, we can use a version-control system such as git to track and save our CLIs, our configuration files, and our workflows (ideally). This set up would also pair nicely with project templates such as Pyscaffoldext-dsproject. We could use: . ./scripts/ to store our CLIs, | ./configs/ to store our configuration files, | ./Makefile to store our workflow definitions (or use whatever common file format we desire), | ./src/{PROJECT_PACKAGE} to store the reusable portion of our project that is used in our CLIs and packaged / shared. | . If we put all these lessons together we arrive at: . version control of our project using a tool like git | a project template such as pyscaffoldext-dsproject | entire project workflow defined as a DAG in plain-text specification (e.g. Makefile, JSON, or YAML) | workflow I/O managed with plain text configuration files (e.g. YAML) | each step of project defined as a function (i.e., a CLI) that takes the configuration as an input. | . These steps alone will take us far towards making our project reproducible. .",
            "url": "https://timothyb0912.github.io/blog/practice/2020/10/24/On-CLIs.html",
            "relUrl": "/practice/2020/10/24/On-CLIs.html",
            "date": " • Oct 24, 2020"
        }
        
    
  
    
        ,"post19": {
            "title": "Project Lifecycle(s)",
            "content": "Project life-cycles in data science and research . Dear Young Tim, . The horrors of an unplanned project . You will work on many projects throughout your life. They will be diverse: research projects, data-science projects, software projects, writing-only projects. Across them all, one truth will remain constant. For each project, you will want to start with (and work through) a plan for completing every step. . Without such a plan, you risk experiencing familiar calamities. Sometimes, you build the wrong thing. In these cases, you have no quick way of finding out whether your proposed idea will work. As a result, you incur costly errors in time, effort, and money from beginning detailed construction without a vetted plan. . At other times, we build the right thing, but we fail to do so in a predictable manner. We face a greater amount of unexpected setbacks than necessary. When we work without a comprehensive1 plan, we add unnecessary uncertainty to our projects. For example, we will be ruefully surprised by any task that we need to complete but fail to expect. This fragility reduces the reliability of our project timeline. As you might imagine, an inability to deliver on time degrades the project experience for us, our stakeholders, and our collaborators. . Lastly, working without a detailed plan and overview of the required tasks is costly. This cost remains even if we manage to build the right thing and complete the project on time. Fundamentally, working without a plan hinders our efforts. In particular, it increases our probability of acting inefficiently. Without a plan, we have a greater chance of taking actions at the wrong time, e.g., premature optimization. Relatedly, we may feel overwhelmed without a plan that specifies a time for each necessary task. And in this state, we may incorrectly try to do too much or too little at once. . Moreover, without a plan, our stakeholders will be full of questions. They will justifiably want to know what is happening in the project and when they can expect particular results. As punishment, you will have to spend your time responding to the random arrival of ~disturbances~ questions. In other words, if you are working without a plan, it means your collaborators and stakeholders are too. . The benefits of planning . Conversely, if we operate with a plan, then we can reap many benefits. For example, we are able to share the plan with our colleagues and stakeholders. The plan can then set our partners’ expectations about what is happening with the project at any time. Additionally, thorough plans have further uses. They reduce the number of unexpected events in our project delivery process. Detailed plans thereby reduce variability and increase the usefulness of our project timelines. And finally, a detailed plan affords us an opportunity to use best practices at every step. . This last point is critical. A thorough project plan breaks the project delivery process into logical steps. After separating and grouping concerns, we frequently consider best practices for these sub-components. E.g., imagine that we’ve identified data processing as one project delivery step. Then, data validation would increase our data processing quality (i.e., trustworthiness). Similarly, imagine that statistical modeling is another step. Then, experiment tracking would increase our modeling quality (i.e., reproducibility). In each case, we use the structure of our plan to intervene on our project to promote the best possible outcome. . Project lifecycle overview . Okay, we’ve described the consequences of working with and without a comprehensive project plan. To ease future planning efforts, we’ll review a common lifecycle for research and data-science projects. At a high-level, such projects entail the following steps. . Observe the current state of your system of interest. | Specify what one wishes to change about this system and what the requirements of one’s users and stakeholders are. | Understand, or learn, why the system does not already behave as one wishes. | Sketch a procedure, a software architecture, and a timeline for effecting your desired system change. | Prototype the sketched design: achieve a minimally acceptable version of one’s desired changes. | Design, in detail, a procedure and software architecture for effecting all desired system changes. | Estimate the amount of time required for construction, and share this information with stakeholders. | Construct the designed architecture, execute the designed procedure, and ensure that we test all and externally review all intermediate steps. | Communicate the constructed architecture, procedure, and educational material. | Maintain the constructed system, collect and act on user feedback, and make continual refactorings for simplicity. | . We’ll describe each of these steps in greater detail below. . Observe . This is where we passively observe our system in action, and we make note of the state or behavior that motivates our project. For example, does our travel mode choice model perform worse for cyclists than for automobile drivers? Is our production model making any systematic errors? Under what conditions do we observe the unwanted system state or behavior? In other words, this first step establishes what the problem or reason for the project is. . Specify . This is the step where we detail the user, technical, and stakeholder requirements of the project. Perhaps most importantly, what effect to do we wish bring about as a result of this project, and what system state should exist upon completion of the project? What are the answers to the who/what/where/when/why/high-level-how questions of our project? For instance, . What requirements do we need to meet to ensure the end user of our project achieves the desired state or behavior? What inputs must we take from users? | What outputs must we provide for users? | What contexts and characteristics of the users do we need to account for? | . | What technical requirements must our project fulfill? E.g., are there latency requirements? | user interface requirements? | code quality standards? | testing requirements? | requirements needed for credible causal inference? | statistical requirements? | . | What stakeholder requirements do we need to meet? Who are our stakeholders? What do they care about? | Do we need to meet certain service level agreements (e.g. predictive accuracy)? | What metrics will we use to judge the outcomes, effects, and impact of this project? How will we define the utility of this project, especially relative to the action of keeping the status quo? Do we need to improve particular metrics by certain amounts? | Do we need to produce particular reports and documentation with any specified formats or elements? | Do we need to create services or software packages to programmatically expose our work to others? | What are our budgetary constraints in time and cost for this project? | . | . Understand . In this stage of a project, we aim to reach a shared understanding with our stakeholders. We want to align our understanding of the primary problems and solutions. Specifically, we want to find out what causes the undesired state or behavior of our system. We also want to develop hypotheses about how we can alter the system to achieve our goals. . The work in this stage spans multiple dimensions: engineering, causal, statistical, communicative. E.g., in this stage, we form causal graphs that are plausible to our stakeholders and (if possible) at least some users. Based on one or more agreed upon causal graphs, what interventions do we hypothesize will lead to the project goal? In terms of engineering, this is where we begin making substantive decisions about our data engineering pipeline. We’ll decide what data to gather, what transformations of that data are useful for our problem, and where we should store the refined data. From a statistical perspective, we thoroughly explore our data to understand it and our problem. And, from a communications standpoint, we shift our focus from our problem statement to our (tentative) solution methods. We begin to think of how we would describe the motivations, intuitions, and mechanisms for our proposed solutions. . Sketch . In this part of the process we document and clarify the thinking that began in the last stage. We: . Sketch the architecture of the software. Sketch the data preparation / causal inference / statistical inference / decision analysis workflows. Sketch the testing procedure for all parts of the software and models. Sketch how to assess business / research / end-user value. Sketch the outline of any communication materials. Sketch the project timeline. . Prototype . In the prototyping phase of one’s project, we have options. At minimum, we build a minimal working prototype of whatever software, scientific, and writing products we need. However, we may take actions such as building multiple types of prototypes to help ourselves answer differing questions. . Some words about tools are in order here. For prototyping causal and statistical inference procedures, Jupyter notebooks will likely be your tool of choice. You’ll typically want to estimate at least one statistical model that appears reasonable. You’ll want to produce one causal graph that appears reasonable, and go all the way through a heavily caveated causal (i.e., counterfactual) simulation. For writing projects, the prototype is typically the outline and the rough draft. For software projects, prototyping typically involves building out some functionalities and meeting some requirements but not others. Business-wise, prototyping often means building out at least one functionality to the point that users can use it and provide feedback. . Design . Once the prototype is complete, you will be in a much better state to understand what is truly needed to fulfill the requirements of one’s projects. At this point, once should create a detailed design and set of plans for iterating to the first complete version of one’s project: the first version that meets all specified requirements. Designing should happen at multiple levels. In particular, one should detail how the project should function internally to produce the desired outputs. . From the perspective of having to write code for one’s project, the following elements should always be part of one’s design. One should pay purposefully plan the architecture of one’s code. What are the major “objects” of one’s system? What data should they store and what behaviors should they have? How should these objects relate to each other? . In tandem, one should plan a testing strategy to make sure the code functions as intended and meets all requirements. Note of course that testing should take place on various levels. For example, one should test: . individual functions and methods with unit tests, | data to make sure it conforms to expectations and that we process it correctly, | conformance with user requirements through acceptance tests and behavior driven development, | the project’s inference methods with its datasets to ensure that one produces sound inferences and that one understands the limitations of one’s techniques in context, | any estimated models through integration tests that check for counterfactual and inferential sensibility of model results, | the building of all communications materials, | for graceful error handling. | . And if we’re being truthful about how frequently we make errors, we will also include a design for how we will refactor and fix bugs. What metrics will we track about our source code to help ensure code quality, identify code smells, and guide refactoring? How will we make sure our code follows and uses relevant (compositions of) design patterns? How will we perform such tracking automatically? How will we make it easy for ourselves and others to understand the current design of our code so they can help refactor? . A similar set of concerns exist with our inferential activities. We should carefully design our causal inference and statistical workflow, from end to end. Given that this workflow will have a matching computational graph, we should be specific about each step’s inputs and outputs. This workflow should include: . data engineering pipeline | exploratory data analysis, | (causal) model specification, | prior predictive checks, | statistical testing of our causal model, | statistical testing of our inferential techniques using prior predictive simulation, | model estimation / inference, | model iteration, | experiment tracking, | model evaluation, posterior predictive checks | cross-validation | model-explanation | model-exploration | . | counterfactual (decision) analysis. In the best case scenario, we would also plan how to analyze the conditions under which our techniques fail to work. How does our technique degrade as we come close to satisfying such conditions? | . Lastly, we need to design our communications process. Since we want to share our work with others, we should design a process that will maximize the chance of successful communication. At minimum, our process should include: . sketching out initial communication ideas, | creating first outlines, | producing communication drafts, | editing our materials, | producing the materials (e.g. building website, docs, article, etc.), | circulating the final documents. | . Additionally, we may need to repeat our communications process across audiences and materials. E.g., people who may later edit or read our code need technical documentation. People who wish to understand the details of our analysis need scientific reports. People who briefly engage our work need high-level overviews of our main findings. Servers that will programmatically call or import our work need APIs, HTTP endpoints, and data infrastructure. So on and so forth. . Estimate . Once we’ve created a detailed design for our project, we can begin estimating how long it will take us to complete it. We can create a project timeline. At this point, I will merely give general guidance here and point you to others who have thought about this much more than myself. . First, it is best to under-promise and over deliver. In other words, be conservative in one’s project timeline estimation. There are always known unknowns. These are the tasks that we know we cannot complete immediately, without reference to outside sources. Then there are unknown unknowns. These are truly unforseen difficulties that slow down one’s efforts. Unfortunately, unknown unknowns happen at all time scales: months, weeks, days, and hours. Accordingly, to deal with unknown unknowns, include estimates for frequent and unplanned time blocks of varying lengths (hours, days, weeks, and months). . Second, it helps to have some rules of thumb when doing project estimation. For week-long or longer projects, I find it helpful to multiply my original (i.e., optimistic) time estimate by 3. This adjustment increased timeline accuracy, especially for tasks I have not completed before. For shorter efforts on the order of an hour or two, I have found that 1 clock-hour typically corresponds to a planned 20-30 minutes worth of 1-2 minute actions. Inevitably some of the short actions take longer than expected and I end up with an elapsed hour. . Third, one should expect one’s schedule to change. Plan for this. One should store one’s schedule in a format that facilitates rapid change. Recently, I’ve been using spreadsheet programs that allow me to create a new sheet whenever I have a large change to the schedule. I change existing cells for small schedule changes. Find what works for you! . Fourth, plan on having a steady stream of outputs for one’s stakeholders. One should plan one’s projects in waves such that on a regular cadence one has full implementations of various desired features. Starting from a prototype, one should implement in vertical efforts that result in one complete product iteration at a time. This will ensure your stakeholders are always in a position where they have a product to use and evaluate. The alternative is developing horizontally. Here, one’s project is built incrementally and one goes from zero finished features to everything all at once. This strategy leaves one’s stakeholders frustrated at being “in the dark” while you tinker for long periods of time. . Fifth, pay attention to resource availability. Will one have access to the resources one needs at all points in the project or does one have time-varying resource constraints? One’s own time may not have a constant rate of availability. The availability of one’s collaborators may not be constant throughout the project. Find out as many of these constraints as possible and account for them in one’s estimation and planning. . Construct . Once you have a detailed design and a publicly shared project-plan, you can begin construction. This is an iterative process of testing, coding, estimating, evaluating, revising, refactoring, and communication. Feature by feature, one transforms the prototype into a product that meets all of the specified project requirements. . The idea for this stage of development is to follow test-driven development practices. One should start by writing a failing test that reflects the requirement that you will write code to fulfill. For direct tests of whether user requirements are fulfilled, it is a good idea to write behavioral tests in the human-readable Gherkin language. For tests of internal functionality, make use of unit tests as usual. In either case, first write a failing test, then write the code to pass the test. . Once our code passes all the new tests, we’ve implemented the new functionality! Great. Now, we should use the feature to produce business or scientific value. I.e., conduct statistical, causal, and decision analyses. We should make the substantive assessments and predictions that motivated the project. The idea is to bring the project to a point that one’s stakeholders will care about as quickly as possible. Swift results will comfort your stakeholders and increase your credibility. Additionally, quick iteration speeds increase one’s opportunities to assess and course-correct one’s work. . After producing preliminary results, we should take the opportunity to refactor our code. Think about how to simplify our software. Check our adherence to design principles. Check to see if any compositions of design patterns fulfills our code’s purposes. And perform static analyses on our code to see if we’ve introduced unnecessary complexity or coupling. Analyze our class diagrams, sequence diagrams, source code metrics. . Lastly, don’t forget to communicate during the construction. This should be taking place at multiple levels. You should be writing documentation for your code in the form of comments, docstrings, and examples. You should be updating your stakeholders on major delays that occur. You should be documenting the results of your analyses as they are so far. Do not wait until the end of one’s project to begin describing one’s methods and results. . Communicate . We should communicate our results and findings continuously, but especially once we have finished a feature or project version. I consider communication to include actions such as: . updating online documentation for the software we’ve built; | preparing reports that detail our findings; | preparing web-interfaces to let stakeholders use our tools; and | preparing packages and HTTP endpoints so users can programmatically access our work. | . Note that communication involves many different tasks. Accordingly, it’s important to use an automated workflow to manage this process. Specifically, use a continuous integration system to ensure that at all times (e.g. nightly or upon merge to our stable branch), one is able to build the documentation, report, and user-interface for one’s project. Through continuous preparation, we reduce the stress incurred when trying to share our work. Moreover, by preparing our materials, we increase the likelihood of successful sharing. Helpful tools for these tasks include . Sphinx and Docusaurus for online documentation, | Jupyter Book and Latex for reports, | Landslide and Auditorium for creating presentations from markdown, | Streamlit for online user-interfaces, | FastAPI to enable programmatic communication through REST APIs, | CML and TravisCI for continuous integration through Github Actions and Travis respectively. | . Maintain . Once we have released a working version of our project for stakeholders to interact with, we should maintain and improve our software. As with all the other steps in the project lifecycle, this step takes place at multiple scales. At the most basic level, we should be able to reproduce our analysis and use the software developed as part of the project. We can verify this with the continuous integration techniques described in the last section. Whenever the continuous integration pipeline raises an error because it cannot complete its tests, we should update our project to fix these issues. . At a higher level, we can maintain the software by continually refactoring to simplify the design of the code while preserving functionality. This level of maintenance is a preventive action. Refactoring makes bugs (i.e., errors in one’s construction and/or design) easier to fix when they happen and easier to avoid in general. Moreover, refactoring leaves the code in a simpler state than it statrted. Other developers or researchers can then leverage this simplicity to extend the software for other uses. . Finally, one should respond to bug reports when other users discover them. Bug fixes will make one’s code even better by repairing previous incorrect or undesired behavior, potentially even giving one’s code new capabilities. . Footnotes . Comprehensive in this case includes some amount of “buffer time” to deal with unknown, but expected complications or problems. &#8617; . |",
            "url": "https://timothyb0912.github.io/blog/philosophy/2020/09/30/Project-Lifecycle.html",
            "relUrl": "/philosophy/2020/09/30/Project-Lifecycle.html",
            "date": " • Sep 30, 2020"
        }
        
    
  
    
        ,"post20": {
            "title": "Prior Visualization",
            "content": "Visualizing Prior Distributions . Dear Young Tim, . My last post to you was about priors for neural networks. Far from being out of the blue, that post was a product of two active streams of research. Specifically, I referenced deep learning in computer science and prior specification from statistics. In doing so, I made two main points. First, neural networks are a flexible way to parameterize one’s likelihood function. And second, we should complement neural network parameters with prior distributions. These priors provide regularization and enable posterior inference / uncertainty estimates for the parameters. . In this post, I’m going to focus on the statistician’s perspective. The broader stream of literature in statistics has converged on the following two points. First, instead of being uninformative as intended, vague / diffuse priors can be highly informative about functionals of one’s model, in ways that one may actively dislike. Second, the way to understand one’s prior is by spending a lot of time inspecting (i.e., visualizing) the prior predictive distribution. . This post will demonstrate the two points above by way of pictures and an example. For relevant reading that provides much of the inspiration for this post, see the following papers: . Seaman III, John W., John W. Seaman Jr, and James D. Stamey. “Hidden dangers of specifying noninformative priors.” The American Statistician 66, no. 2 (2012): 77-84. | Gelman, Andrew, Daniel Simpson, and Michael Betancourt. “The prior can often only be understood in the context of the likelihood.” Entropy 19, no. 10 (2017): 555. | Gabry, Jonah, Daniel Simpson, Aki Vehtari, Michael Betancourt, and Andrew Gelman. “Visualization in Bayesian workflow.” Journal of the Royal Statistical Society: Series A (Statistics in Society) 182, no. 2 (2019): 389-402. | . The conclusion for one’s own research is that when building models of any kind, one should spend time investigating and setting informative priors for one’s models by visualizing the prior predictive distribution. . Below, I’ll share some examples of such prior investigations using the widely available Boston Housing data set: . Harrison Jr, David, and Daniel L. Rubinfeld. “Hedonic housing prices and the demand for clean air.” (1978). . The model that we’ll consider is a folded-logistic regression model where the location is linearly parameterized based on the dataset’s raw features, and where the scale parameter is homogenous and explicitly estimated. Note that if ZZZ follows a logistic distribution with location μ muμ and scale σ sigmaσ, then ∣Z∣ lvert Z rvert∣Z∣ follows a folded logistic distribution. Basically, this is a linear regression where we restrict the support of the outcome (i.e, the median home value) to positive real values, following a logistic distribution bounded below by zero. . How do we do this? . The general strategy for prior predictive checks is as follows. . Specify a prior distribution for all parameters in one’s model. | Sample parameters from the prior. | Simulate outcomes from the model, conditional on one’s sampled parameters. | Visualize select statistics based on the simulated outcomes and explanatory variables. | To streamline this process, we need an interactive way of specifying our priors. While many options for interactivity exist, I’ll make use of the Streamlit package in Python. This package is easy to use, is generally free of “gotchas,” and enables one to set up a user-interface for one’s programs in a short time. . The basic idea is that we’ll select distributions via drop-down menus, and we’ll select parameters for those distributions via sliders. Then, once we’ve made our selections, we can press “Draw” or some other descriptively named button. This button press will trigger execution of a program that we’ve written to sample from our prior predictive distribution and create the desired visualizations. Finally, we’ll use Streamlit to automatically display these visualizations. . What do we check? . For an example of the prior predictive checking process in action, see the following repository: https://github.com/timothyb0912/prior-elicitation/. In particular, the streamlit app that I created to visualize the prior predictive checks is here, and the file I created to simulate from and visualize the prior is here. . To begin, we’ll need to choose some quantities to visualize. Here, we have complete freedom. For a variety of quantities that may prove useful for prior (or posterior) predictive checks, see . Brathwaite, Timothy. “Check yourself before you wreck yourself: Assessing discrete choice models through predictive simulations.” arXiv preprint arXiv:1806.02307 (2018). . For this example, we’ll consider a statistic that was not described in my paper. Specifically, we’ll examine the 25th percentile of Y * X, where X is a selected column from our design matrix. For instance, if we select the column of one’s in the design matrix, then we’ll view the prior predictive distribution of the 25th percentile of Y. . Here’s a picture using a typical diffuse / non-informative prior. Each coefficient for the design matrix has mean zero and standard deviation of 5. The scale parameter has a folded normal prior distribution with location 1 and scale of 1 (yes… meta, I know). From the image, we see that the prior predictive distribution places far too much mass on entirely implausible values (e.g. smaller than the minimum observed median home value or larger than the maximum observed median home value). The non-informative prior on the model parameters is actually quite opinionated (in misrepresentative ways) about our prior beliefs in functionals of our data. . From here, the idea is clear enough: play around! Iteratively adjust the sliders and display your chosen prior predictive checks for each new specification. With enough experimentation, one should be able to create a prior that appears reasonable. . See below for a video clip of how one can use the interactive interface to experiment with different prior settings. . In future posts, I’ll discuss how to set priors programmatically. Clearly, the manual approach described above will not scale to high dimensional settings. We’ll need to do something smarter! . P.S. The test statistic chosen for the prior predictive check has some rationale to it. In a standard linear regression, the first order conditions that one must solve to achieve a gradient of zero is $(Y - hat{Y})X = 0$, where X is one’s design matrix. This is equivalent to saying that the mean of YXYXYX should equal the mean of Y^X hat{Y}XY^X. In other words, the observed mean of YXYXYX should equal the predicted Y^X hat{Y}XY^X after estimating one’s model. Checking that the chosen test statistic of a selected percentile (e.g. 25th percentile) of YXYXYX resembles the distribution of simulated YsimXY_{ textrm{sim}}XYsim​X is a related activity. This helps ensure that more points of the distribution of YXYXYX are well represented by one’s model, beyond the mean of the distribution. .",
            "url": "https://timothyb0912.github.io/blog/simulation/2020/08/31/Prior-Visualization.html",
            "relUrl": "/simulation/2020/08/31/Prior-Visualization.html",
            "date": " • Aug 31, 2020"
        }
        
    
  
    
        ,"post21": {
            "title": "On Priors for Neural Networks",
            "content": "On Priors for Neural Networks . Dear Young Tim, . I hope this note reaches you long before you need it. At some point, you will want to experiment with neural networks. And when you do, you will want to learn both how they work and how you can make them work well. Essentially, this means learning how to design (i) likelihoods, i.e. neural network architectures and activations, (ii) priors, i.e. regularization schemes, and (iii) training / inference procedures. . In this post, I offer you an opinionated introduction to constructing and using neural networks. I come to you as a sort of statistical, Mardi Gras Indian, Spy Boy. Indeed, I have scouted ahead and brought back strange reports of success and despair. . I know our procession is a Bayesian one, a practical and practicing one at that. Accordingly, like any thorough Bayesian model, this story begins with the prior. Not with the main character, the likelihood, the neural network itself. But instead with the background, the prior, the context through which we regularize and understand the entire drama. . In time, you will find that training often becomes easier with a good prior and that using the prior predictive distribution is a great way to understand neural architectures. You will learn that the fastest way to create neural networks that generalize well is to use good priors during training, and you will spend a lot of time and computational resources figuring out how to do so. This post aims to provide you with a framework for thinking about priors / regularization for neural networks. In the end this framework should save you much time, computation, and confusion, but at the moment, let’s begin with the basics. . Helpful priors from Frequentists . The first report I have for you is that Frequentists who build neural networks care a lot about neural network priors. Moreover, Frequentists are commonly successful at constructing priors that enable them to achieve impressive out-of-sample performance. . Yes, of course, you didn’t expect to see such concerns from Frequentists… I know. If it’s any consolation, it’s July 2020 for me. By this point, after the coronavirus pandemic, after the global recession, after the continued racism… little will surprise you anymore. But for now, soak it all in. . To start with, Frequentists will not speak of prior distributions when describing how they construct their neural networks. Instead, they will equivalently speak of regularization methods. However, have no fear. As discussed below, most regularization methods have explicit counterparts in Bayesian priors. . For example, placing a mean zero Gaussian prior over one’s weights (i.e. neural network parameters) leads to L2-weight decay (see Table 1, p.2821 here). Similarly, placing a mean zero Laplace prior over one’s weights leads to L1-weight decay. Perhaps less familiarly, data augmentation is also a way encoding prior information (see for example, here and here). And so on, and so forth. Stay tuned for future posts that provide a more comprehensive list of such comparisons. . For now, it suffices to say that much research on designing good regularization methods for deep learning is implicitly about designing good priors for one’s neural networks. And this brings us to today’s first paper of interest. Do Deep Nets Really Need to be Deep? by Ba and Carauna (2014). . Ba and Carauna’s paper is one of the first to introduce the concept of knowledge/model distillation: taking the knowledge obtained from a given model, called the teacher, and passing that knowledge to a second model, called the student. The teacher packages its knowledge using predictions (or intermediate quantities) on the dataset used to train the student model. A regularization term is then used to penalize the student model for deviations between its predictions or intermediate quantities and those of the teacher. As one might expect given the preceding discussion about regularization often equaling prior specification, placing a prior on the divergence of your neural network’s predictions from those of a teacher’s predictions leads to model distillation. See here for historical examples of such priors on predictive functionals in statistics using “human expert teachers,” and see here for a modern bayesian perspective on model distillation. . The important findings from Ba and Carauna’s paper are the following. . “[M]odels trained on targets predicted by other models can be more accurate than models trained on the original labels” (p. 6) . “The mechanisms above can be seen as forms of regularization that help prevent overfitting in the student model” (p. 7) . “Although there is a consistent performance gap between the two models due to the difference in size, the smaller shallow model was eventually able to achieve a performance comparable to the larger shallow net by learning from a better teacher, and the accuracy of both models continues to increase as teacher accuracy increases. […] We see little evidence that shallow models have limited capacity or representational power. Instead, the main limitation appears to be the learning and regularization procedures used to train the shallow models.” (pp. 7-8) . Or, put differently, the provision of a good prior (in this paper, an accurate teacher model and choice of penalization function) . can be even more important for neural network performance than providing access to the real data (see also here), | helps prevent overfitting, and | might be the primary missing factor needed to construct performant neural networks (at least for some network classes). | . Harmful priors from Bayesians . My second report is that Bayesians have been building neural networks with priors that do more harm than good. . One of the most recent clarion calls was Wenzel et al.’s How Good is the Bayes Posterior in Deep Neural Networks Really? (2019). Wenzel et al.’s motivating observation was that, in the papers they considered and in their experience, tempering the posterior distribution often led to better predictive performance compared to using the actual posterior distribution. The authors view the lackluster performance of the true posterior distribution as a problem because it suggests problems with the likelihood, prior, or training procedure. . To investigate the sources of the posterior distribution’s dismal performance, Wenzel et al. perform a series of computational experiments. Based on these trials, the authors assign high probability to the posterior distribution’s underperformance being due to the prior distributions used with Bayesian neural networks, as opposed to the likelihood or training procedures. . In particular, the authors look at a standard, multivariate normal distribution prior (i.e., L2-weight decay). First, they find that with an increasing number of parameters, the standard normal prior leads to unintentially informative prior predictive distributions. They uncover prior predictive distributions that are much more extreme than desired. . Second, the authors realize that as they decrease the number of observations in their training set, the predictive performance of their posterior distribution degrades faster than the predictive performance of the maximum likelihood estimate (which implicitly assumes an improper uniform distribution). Since the effect of the prior increases as one decreases the number of observations, this indicates that the standard normal prior is worse than the improper uniform prior in the settings they considered. For yet another example of problems arising from using the standard normal prior with neural networks, see the “soap-bubble pathology” (Farquhar et al., 2019). . These observations all stand as examples of “the hidden dangers of noninformative priors” or default priors. Researchers often adopt such priors for computational or political ease, but they may have deleterious effects on one’s posterior performance and inference. In fact, one should perhaps expect such negative consequences given the typical, over-parameterized neural network context with a high ratio of parameters to one’s number of observations. . Conclusion . The papers How Good is the Bayes Posterior in Deep Neural Networks Really? and Do Deep Nets Really Need to be Deep? show that prior distributions are important to neural network construction. These papers show that priors can have either helpful or harmful effects on one’s predictive ability, relative to maximum likelihood estimation. To give oneself the best chance of constructing neural networks that perform well, and to reduce the chance of having our predictive performance harmed unnecessarily, the takeaway is that we should carefully choose the priors for our models. .",
            "url": "https://timothyb0912.github.io/blog/reading/2020/07/29/Neural-Net-Priors.html",
            "relUrl": "/reading/2020/07/29/Neural-Net-Priors.html",
            "date": " • Jul 29, 2020"
        }
        
    
  
    
        ,"post22": {
            "title": "PyTorch Tutorial",
            "content": "PyTorch Tutorial . Dear Young Tim, . I know you remember the good old days: writing out probability equations, analytically deriving gradients and hessians of the log-likelihood, and then painstakingly implementing those derivatives in code. . Throughout your career, especially in school, you will forego research efforts because the time to analyze and implement the derivatives seemed greater than the gain from doing so. . Gallery of asymmetric choice models? No. Gradient-based MCMC for your decision tree + choice model paper? No. Mixed Logit models with non-normal mixing distributions in PyLogit? No. . Despite the good reasons for my decisions, loss aversion and questions of “what-if” will plague you. . Moreover, this behavioral pattern will persist even after you graduate from UC Berkeley. Somehow, you will find yourself with less free time and more competing activities that you wish to fill that time with. Every year you will say more “no’s” as you ruthlessly triage new research ideas based, in part, on their projected execution time. Your feeling of loss due to time constraints will increase. . Far from coming to engage in a pity party, I’m here to announce that there is hope for change. You can stop this wound from bleeding. Across projects, you can lower projected execution times and their variance. Unshackle yourself from some feelings of constraint-based loss by eliminating time for gradient and hessian derivations. . A technique called automatic differentiation enables automatic gradient and hessian computation once you write the code to compute one’s log-likelihood and probabilities. PyTorch and Tensorflow began supporting sparse matrices in 2017 and 2016, respectively, making it possible to use these packages’ automatic differentiation tools for discrete choice models. In other words, discrete choice modellers could have been living the dream for three to four years! No more passing up on research ideas because derivations take too long. Write your log-likelihood and probability functions in PyTorch or Tensorflow, and you’ll get the exact gradients and hessians automatically! . Of course, despite the enabling technology existing, uptake of these tools has been slow. For example, I began using PyTorch in 2019, after failed attempts in 2018. My biggest stumbling block was not having a comprehensive understanding of what I needed and how I would interface with other third-party packages (NumPy, SciPy, etc.). How could I use these packages to do something I already knew how to do? I found Tensorflow’s learning curve to be terribly steep–so steep that I never came back to it! PyTorch was much easier for me to begin with, so that’s what this post is about. . I hope this post will hasten your adoption of automatic differentiation. Below, I describe the high-level elements you’ll need when replacing one’s hand-coded derivatives with PyTorch computed derivatives. Moreover, I introduce PyTorch in a non-trivial discrete choice setting, replicating the Mixed Logit “B” model of . Brownstone, David, and Kenneth Train. “Forecasting new product penetration with flexible substitution patterns.” Journal of econometrics 89, no. 1-2 (1998): 109-129. . To be clear, this is not merely a “toy” exercise for demonstration. I am doing this work to revise my paper . Brathwaite, Timothy. “Check yourself before you wreck yourself: Assessing discrete choice models through predictive simulations.” arXiv preprint arXiv:1806.02307 (2018). . After reading this post, you should be able to begin estimating whatever other model you think of. . With this context in mind, let’s get started! . Overview . To keep matters simple, I’ll describe how to use PyTorch to perform maximum likelihood estimation of one’s discrete choice model. The expectation is that, as in PyLogit, you’ll use scipy.optimize.minimize for parameter optimization. In this setting, the following five objects are sufficient for getting started. . A loss function written in PyTorch. | An input object containing all data needed to compute probabilities for the choice observations of interest. | A PyTorch model (i.e. a subclass of torch.nn.Module) that packages all model parameters for optimization and the probability computation logic. | Helper functions to get the gradient and hessian of the log-likelihood as Numpy arrays for use with scipy.optimize.minimize. | A closure for scipy.optimize.minimize that takes in a passed array of parameters and returns the loss and gradient of the loss. | Let’s import the needed modules for this notebook and then look at some examples of the five objects listed above. . Example . # Py3 Built-in module for type-hint annotations from typing import Dict, Tuple, List, Callable, Optional # Third-party modules for: # class definition import attr # Optimization import scipy.optimize as opt # Numerical computation import torch import torch.nn as nn import numpy as np # Utility functions for pytorch optimization import botorch.optim.numpy_converter as numpy_converter import botorch.optim.utils as optim_utils from botorch.optim.numpy_converter import TorchAttr # Input/Output of data import pandas as pd . Loss Function . The simplest of the objects listed in the overview is the loss function. At its core, the loss function is a python function that returns a scalar based (even if only indirectly) on the parameters being optimized. . Here is an example of the loss-function I typically use when estimating discrete choice models: the log-loss, i.e. the negative of the log-likelihood. . If this is your first time using PyTorch, then one thing to recognize is that torch.Tensor is PyTorch’s equivalent of Numpy’s ndarray. PyTorch Tensors and Numpy arrays share the same locations in your computer’s memory, and converting between these two data containers is memory-efficient (i.e., without data-copying). . def log_loss(probs: torch.Tensor, targets: torch.Tensor) -&gt; torch.Tensor: &quot;&quot;&quot; Computes the log-loss (i.e., the negative log-likelihood) for given long-format tensors of probabilities and choice indicators. Parameters - probs : 1D torch.Tensor. The probability of choosing each row&#39;s alternative for the given choice situation. targets : 1D torch.Tensor. A Tensor of zeros and ones indicating the chosen row for each choice situation. Should have the same size as `probs`. Returns - neg_log_likelihood : scalar torch.Tensor. The negative log-likelihood computed from `probs` and `targets`. &quot;&quot;&quot; log_likelihood = torch.sum(targets * torch.log(probs)) return -1 * log_likelihood . Input Object . Many interrelated objects are often needed to calculate discrete choice model probabilities. For instance, PyLogit’s multinomial logit models rely not just on a design matrix but on multiple sparse “mapping matrices” that relate rows of the design matrix to observation or alternative identifiers. Another example is the mixed logit model. It relies on a set of generated normal random variates for each observation. Because these randomly drawn values are to remain constant during one’s estimation process, they need to be passed into one’s probability calculating function along with the design matrix that one typically thinks of when using a predict function. . The input object packages all these related objects needed to compute probabilities. The official PyTorch method for packaging these objects would be to use the following classes: . torch.utils.data.DataLoader | torch.utils.data.Dataset | torch.utils.data.IterableDataset | torch.utils.data.Sampler (or subclasses) | . However, these PyTorch objects seemed more complicated than necessary for estimating a discrete-choice model using batch-optimization methods (instead of mini-batch optimization techniques). The input object below is, in my opinion, a simpler data class that still provides the needed packaging functionality. . @attr.s class InputMixlB: &quot;&quot;&quot; Stores the observation-specific data needed to compute probabilities in the Mixed Logit B model. &quot;&quot;&quot; # Needed attributes are: # design matrix design: torch.Tensor = attr.ib() # rows_to_choice_situation, i.e, rows_to_obs obs_mapping: torch.sparse.FloatTensor = attr.ib() # rows_to_decision_makers, i.e, rows_to_mixers mixing_mapping: torch.sparse.FloatTensor = attr.ib() # list of normal random variates normal_rvs: List[torch.Tensor] = attr.ib() @classmethod def from_df(cls, df: pd.DataFrame, mixing_seed: int=601, num_draws: int=250) -&gt; &#39;InputMixlB&#39;: &quot;&quot;&quot; Creates a class instance from a dataframe with the requisite data. Parameters - df : pandas DataFrame. Should be a long-format dataframe containing the following columns: `[alt_id, obs_id, choice]`. mixing_seed : optional, int. Denotes the random seed to use when generating the normal random variates for Monte Carlo integration in the maximum simulated likelihood procedure. num_draws : optional, int. Denotes the number of random draws to use for Monte Carlo integration in the maximum simulated likelihood procedure. Returns - Instantiated &#39;InputMixlB&#39; object. &quot;&quot;&quot; ... . Let me explain what is going on here. . First, I am using the attr.s decorator to signal that I want to create instances of this class using the attr library. This library has multiple benefits, but the most relevant ones in this example are that I can avoid writing an __init__ function and I can access data using human readable property names. Next, I use attr.ib() to declare the attributes of the class. Using PyLogit terminology, I am storing the design matrix, the rows_to_choice_situation and rows_to_decision_makers mapping matrices, and the normal random variates for estimating the maximum simulated likelihood. . After declaring the class attributes, I define a class method to initialize an instance of the class. This is denoted by the @classmethod decorator. The method, although not displayed above for brevity, encapsulates the logic (all 56 lines of it) to take a given dataframe of data for the Mixed Logit Model B and create the class attributes described above. The way to use it is by typing mixlb_input = InputMixlB.from_df(df) where df is as described in the from_df docstring. See here for an example of the from_df method in use and here for the full definition of the method. . The main point is that once you have an input object, you can pass the object or the object’s attributes to the forward method of your PyTorch model to compute the needed probabilities. Note that all the attributes are pytorch objects. These are the only type of objects that one should pass to your model’s forward method if one wants to use automatic differentiation on the resulting log-likelihood. . PyTorch Model . The most obviously needed object for using PyTorch’s automatic differentiation capabilities is a PyTorch model. In particular, one’s PyTorch model must be a subclass of nn.Module that overwrites / implements the forward method. Since we’re estimating discrete choice models, the forward method should take an input object or that object’s attributes, and the forward method should output a 1D Tensor of predicted probabilities in “long-format.” . Note that one’s PyTorch model should do more than capture the logic needed to compute the discrete choice model’s probabilities. One’s PyTorch model should also store any data needed to compute the probabilities that does not change across observations. . As an example, let’s look at the skeleton of the PyTorch model used to compute the probabilities for Brownstone and Train’s (1998) Mixed Logit B model. . @attr.s class DesignInfoMixlB: &quot;&quot;&quot;Put generic information about the design matrix here.&quot;&quot;&quot; ... @attr.s(eq=False, repr=False) class MIXLB(nn.Module): &quot;&quot;&quot;&quot; PyTorch implementation of `Mixed Logit B` in [1]. References - [1] Brownstone, David, and Kenneth Train. &quot;Forecasting new product penetration with flexible substitution patterns.&quot; Journal of econometrics 89.1-2 (1998): 109-129. &quot;&quot;&quot; # Should store all needed information required to specifcy the # computational steps needed to calculate the probability function # corresponding to `Mixed Logit B` # Info denoting the design columns, the indices for normal and lognormally # distributed parameters, etc. design_info = attr.ib(init=False, default=DesignInfoMixlB()) # Standard deviation constant for lognormally distributed values log_normal_std = attr.ib(init=False, default=torch.tensor(0.8326)) #### # Needed constants for numerical stability #### # Minimum and maximum value that should be exponentiated min_exponent_val = attr.ib(init=False, default=torch.tensor(-700)) max_exponent_val = attr.ib(init=False, default=torch.tensor(700)) # MNL models and generalizations only have probability = 1 # when the linear predictor = infinity max_prob_value = attr.ib(init=False, default=torch.tensor(1-1e-16)) # MNL models and generalizations only have probability = 0 # when the linear predictor = -infinity min_prob_value = attr.ib(init=False, default=torch.tensor(1e-40)) def __attrs_post_init__(self): # Make sure that we call the constructor method of nn.Module to # initialize pytorch specific parameters, as they are not automatically # initialized by attrs super().__init__() # Needed paramters tensors for the module: # - parameter &quot;means&quot; and &quot;standard deviations&quot; self.means = nn.Parameter(torch.arange(len(self.design_info.column_names), dtype=torch.double)) self.std_deviations = nn.Parameter(torch.arange(len(self.design_info.normal_coef_names), dtype=torch.double)) # Enforce parameter constraints self.constrain_means() def constrain_means(self): &quot;&quot;&quot; Ensures that we don&#39;t compute the gradients for mean parameters that should be constrained to zero. &quot;&quot;&quot; # Note that parameters 21 (non_ev) and 22 (non_cng) are constrained to # zero because those columns are for random effects with mean zero by # specification. self.constrained_means = optim_utils.fix_features(self.means, {21: None, 22: None}) def forward(self, design_2d: torch.Tensor, rows_to_obs: torch.sparse.FloatTensor, rows_to_mixers: torch.sparse.FloatTensor, normal_rvs_list: List[torch.Tensor]) -&gt; torch.Tensor: &quot;&quot;&quot; Compute the probabilities for `Mixed Logit B`. Parameters - design_2d : 2D torch.Tensor. Denotes the design matrix whose coefficients are to be computed. rows_to_obs : 2D torch.sparse.FloatTensor. Denotes the mapping between rows of `design_2d` and the choice observations the probabilities are being computed for. rows_to_mixers : 2D torch.sparse.FloatTensor. Denotes the mapping between rows of `design_2d` and the decision-makers the coefficients are randomly distributed over. normal_rvs_list : list of 2D torch.Tensor. Should have length `self.design_info.num_mixing_vars`. Each element of the list should be of shape `(rows_to_mixers.size()[1], num_draws)`. Each element should represent a draw from a standard normal distribution. Returns - average_probabilities : 1D torch.Tensor Denotes the average of the probabilities for each alternative for each choice situation, across all random draws of the model coefficients. &quot;&quot;&quot; # Get the coefficient tensor for all observations # This will be a 3D tensor coefficients = self.create_coef_tensor(design_2d, rows_to_mixers, normal_rvs_list) # Compute the long-format systematic utilities per row and random draw. # This will be a 2D tensor systematic_utilities = self._calc_systematic_utilities(design_2d, coefficients) # Compute the long-format probabilities for each row and random draw. # This will be a 2D tensor probabilities_per_draw = self._calc_probs_per_draw(systematic_utilities, rows_to_obs) # Compute the long-format, average probabilities across draws. # This will be a 1D tensor. average_probabilities = torch.mean(probabilities_per_draw, 1) return average_probabilities def create_coef_tensor( self, design_2d: torch.Tensor, rows_to_mixers: torch.sparse.FloatTensor, normal_rvs_list: List[torch.Tensor] ) -&gt; torch.Tensor: ... def _calc_systematic_utilities(self, design_2d: torch.Tensor, coefs: torch.Tensor) -&gt; torch.Tensor: ... def _calc_probs_per_draw( self, sys_utilities: torch.Tensor, rows_to_obs: torch.sparse.FloatTensor ) -&gt; torch.Tensor: ... . The model is the most complex object described in this post so let’s spend some time unpacking what is going on above. . The big picture is as follows. The code snippet above subclasses nn.Module and implements the forward method of the model to compute the choice probabilities. These are the important steps shown above. Everything else is an implementation detail. Now, I’ll proceed chronologically from the top of the cell and explain more of these specifics. . The first subtle action is how I invoke the attr.s decorator. Unlike the case of the InputMixlB object, I now pass the decorator arguments eq=False, repr=False. Both arguments are necessary for PyTorch and the attr module to operate nicely together. The eq=False argument allows hashing of the nn.Module and thereby permits use of the module’s internal C++ functions. The repr=False argument tells attr not to overwrite the default string representation given by PyTorch’s nn.Module class. For more information about why these workarounds are necessary, see here. . Next, I define two model specific attributes: design_info and log_normal_std. The design_info attribute contains: . attributes denoting the column names of the design matrix, | the column indices for normal and log-normally distributed parameters, and | mappings relating the design columns to the lognormal or normally distributed coefficient names. | . The log_normal_std attribute is a constant denoting the standard deviation of the normal random variates used in the log-normal mixing distributions of the Mixed Logit B model. The common feature of these two attributes is that they both represent model-specific information that does not change across observations. I therefore stored them on the model object itself. . After these, the next four attributes are minimum and maximum values to be exponentiated and treated as probabilities, respectively. These attributes are not model-specific, as I tend to use them across all my discrete choice models to avoid underflow and overflow. However, since these values do not change across observations, I also stored them on the model object. . Note, while defining these attributes, I passed init=False and default=something. The init=False keyword argument tells attr that these attributes do not need to be passed when initializing an instance of the model class. The default=something keyword argument tells attr that if these attributes are not passed as arguments to the __init__ function, then use something as the value of the attribute instead (whatever something is). . Now, with the model-specific attributes stored as attributes, the PyTorch-specific attributes become the center of attention. To set these attributes, I use the __attrs_post_init__ function, which is called after executing the __init__ function of the MIXLB object. This __attrs_post_init__ function, as defined above, will then call the __init__ function of nn.Module (via super().__init__()) to perform PyTorch specific initialization actions. Then, I assign the nn.Parameter attributes means and std_deviations. The Mixed Logit B model optimizes these parameters during estimation. Finally, I use the PyTorch enhancing BoTorch.optim.utils module to enforce the model-specific parameter constraints using the call self.constrain_means. I emphasize this step to point out that arbitrary parameter constraints can be imposed on one’s model. . At long last, we come to the forward method. As noted before, this method encapsulates the logic needed to compute the probabilities for Mixed Logit B. Importantly, this logic is not all written in one monolithic forward method. Because the model relies on a set of intricate sub-steps, especially create_coef_tensor, these substeps are defined as their own methods on the MIXLB class. These methods ease the reading of the forward method and clarify its logic. Note that for the sake of brevity, the sub-step methods are not displayed above. See here for the complete definition of the sub-step methods and of the class in general. . To be completely honest, it’s not only the sub-step methods that were hidden from display above. To avoid displaying a long cell, I also hid the “helper” functions that enable painless interfacing between Numpy, Scipy, and PyTorch. Let’s look at these helper functions below. . Helper functions . PyTorch and Numpy/Scipy interact in four main situations: . setting a Numpy array of parameters on a PyTorch model | getting a Numpy array of parameters from a PyTorch model | getting a Numpy array of parameter gradients from a PyTorch model | getting a Numpy array of the hessian of one’s PyTorch model parameters. | . The helper functions below address the first three of these situations. . @attr.s(eq=False, repr=False) class MIXLB(nn.Module): &quot;&quot;&quot;&quot; PyTorch implementation of `Mixed Logit B` in [1]. References - [1] Brownstone, David, and Kenneth Train. &quot;Forecasting new product penetration with flexible substitution patterns.&quot; Journal of econometrics 89.1-2 (1998): 109-129. &quot;&quot;&quot; ... def get_params_numpy(self) -&gt; Tuple[ np.ndarray, Dict[str, TorchAttr], Optional[np.ndarray]]: &quot;&quot;&quot; Syntatic sugar for `botorch.optim.numpy_converter.module_to_array`. Returns - param_array : 1D np.ndarray Model parameters values. param_dict : dict. String representations of parameter names are keys, and the values are TorchAttr objects containing shape, dtype, and device information about the correpsonding pytorch tensors. bounds : optional, np.ndarray or None. If at least one parameter has bounds, then these are returned as a 2D ndarray representing the bounds for each paramaeter. Otherwise None. &quot;&quot;&quot; return numpy_converter.module_to_array(self) def set_params_numpy(self, new_param_array: torch.Tensor) -&gt; None: &quot;&quot;&quot; Sets the model&#39;s parameters using the values in `new_param_array`. Parameters - new_param_array : 1D ndarray. Should have one element for each element of the tensors in `self.parameters`. Returns - None. &quot;&quot;&quot; # Get the property dictionary for this module _, property_dict, _ = self.get_params_numpy() # Set the parameters numpy_converter.set_params_with_array( self, new_param_array, property_dict) # Constrain the parameters self.constrain_means() def get_grad_numpy(self) -&gt; None: &quot;&quot;&quot; Returns the gradient of the model parameters as a 1D numpy array. &quot;&quot;&quot; grad = np.concatenate(list(x.grad.data.numpy().ravel() for x in self.parameters()), axis=0) return grad . Luckily, each of the methods above are simple. get_params_numpy and set_params_numpy provide accessors to BoTorch’s pre-existing functions, each requiring no more than three lines of code. The get_grad_numpy function uses natively available PyTorch attributes and methods in a single assignment statement. . The fourth situation, computing the hessian, did not appear to be accessible solely via the model object after calculation of the loss. It seemed that no matter what, I would need to pass both the model (or its attributes) as well as the loss to a function in order to compute the hessian. I could be wrong about this as I have not investigated fully! Nevertheless, given my impressions from limited information so far, it seemed inappropriate to create a method on the model object that computed the hessian. I left this task to be explicitly performed by the user outside of a model method. . Now, with all the objects and methods described so far, we can finally optimize our model’s parameters according to our chosen loss function. . Let’s create one last object below, the optimization closure, to help us perform precisely this optimization. . Optimization closure . The purpose of creating an optimization closure is to create an objective function that is compatible with scipy.optimize.minimize. Scipy describes the needed optimization function with the following statements. First, Scipy specifies that the optimization function should take a 1D array of parameters as inputs and return a floating point scalar as output. . fun ; callable . The objective function to be minimized. fun(x, *args) -&gt; float where x is an 1-D array with shape (n,) and args is a tuple of the fixed parameters needed to completely specify the function. . Next, we extract a second set of requirements for our optimization function by considering the fact that we wish to perform gradient-based optimization. To use gradient-based optimization, we have to provide the gradient, i.e., the jacobian for 1D objective function outputs. It is important to note that Scipy allows the gradient to be provided in one of two ways. One way is that we can provide a callable, separate from the optimization function fun, that will compute the gradient. The second way is that we provide the gradient as an output of the optimization function. . This requirement is described as follows. . jac ; {callable, ‘2-point’, ‘3-point’, ‘cs’, bool}, optional . […] If jac is a Boolean and is True, fun is assumed to return and objective and gradient as and (f, g) tuple. . For computational efficiency, it makes sense to avoid repeating oneself. In particular, since many operations in computing the gradient are performed when computing the loss, we should avoid computing the loss twice. Instead, we will compute the loss once (to be returned as float from fun) and we will immediately compute the gradient for return. This is shown in the closure below. . def make_scipy_closure( input_obj: InputMixlB, targets: torch.Tensor, model: MIXLB, loss_func: Callable, ) -&gt; Callable: &quot;&quot;&quot; Creates the optimization function for use with scipy.optimize.minimize. Parameters - input_obj : InputMixlB. Container of the inputs for the model&#39;s probability function. targets : 1D torch.Tensor A Tensor of zeros and ones indicating which row was chosen for each choice situation. Should have the same size as `(input_obj.design.size()[0],)`. model : MIXLB. Should have a forward object that computes the probabilities of the given discrete choice model. loss_func : callable. Should take as inputs, `model` outputs and `targets`. Should return the value of the loss as well as the gradient of the loss. Returns - optimization_func : callable Takes a set of parameters as a 1D numpy array and returns the corresponding loss function value and gradient corresponding to the passed parameters. &quot;&quot;&quot; def closure(params): # params -&gt; loss, grad # Load the parameters onto the model model.set_params_numpy(params) # Ensure the gradients are summed starting from zero model.zero_grad() # Calculate the probabilities probabilities = model(design_2d=input_obj.design, rows_to_obs=input_obj.obs_mapping, rows_to_mixers=input_obj.mixing_mapping, normal_rvs_list=input_obj.normal_rvs) # Calculate the loss loss = loss_func(probabilities, targets) # Compute the gradient. loss.backward() # Get the gradient. grad = model.get_grad_numpy() # Get a float version of the loss for scipy. loss_val = loss.item() return loss_val, grad return closure . Of course, in your own projects, you’ll want to replace the input_obj, model, and loss_func with the custom objects you’ve created. Examples of creating these objects are provided above! . Conclusions: Putting it all together . Alright, the five objects above provide all one needs for optimizing the parameters of your model. Within the scipy.optimize.minimize function, you’ll pass the output of make_scipy_closure as fun and you’ll pass jac=True. That should be all! . The resulting optimization routine should roughly look as follows: . # Load the data for the model. df = pd.read_csv(DATA_PATH) # Instantiate the model inputs. input_mixlb = InputMixlB.from_df(df) # Instantiate the model. mixlb_model = MIXLB() # Create pytorch targets with 32-bit precision. choice_tensor = torch.from_numpy(df[CHOICE_COLUMN].values.astype(np.float32)) # Choose initial values for the optimization. initial_values = np.zeros(mixlb_model.get_params_numpy().size, dtype=float) # Create the optimization function for scipy scipy_objective = make_scipy_closure(input_obj=input_mixlb, targets=choice_tensor, model=mixlb_model, loss_func=log_loss) # Perform the parameter optimization. # Use your method of choice. optimization_results = opt.minimize(scipy_objective, initial_values, jac=True, method=&#39;bfgs&#39;) . To see such an approach in action, check out this file in the “Check Yourself Before You Wreck Yourself” Github repository. In that file, you can also see how the hessian is computed (relevant lines here). It’s pretty simple, so I’ve excluded it from this already long post. . Now, go forth and estimate models with abandon! .",
            "url": "https://timothyb0912.github.io/blog/practice/2020/06/25/PyTorch-Tutorial.html",
            "relUrl": "/practice/2020/06/25/PyTorch-Tutorial.html",
            "date": " • Jun 25, 2020"
        }
        
    
  
    
        ,"post23": {
            "title": "On good models and good research",
            "content": "On good models and good research . Dear Young Tim, . If I know you, and I’m pretty sure I do, then you will want to maximize the amount of “great work” that you produce in your life. . You have read all about Richard Hamming’s (1986) calls to “drop modesty and say to yourself ‘[y]es, I would like to do first class work’.” You have read Cal Newport’s (2012) treatise on how a good life is made more probable by doing work that is “so good, they can’t ignore you.” . By now, you have bought into the idea, and you are ready to try your hand at doing the best scholarly / scientific work you can. Wonderful. The problem, however, is that you have not done any great work yet. You don’t know how to do it, and you’ll be figuring it out for a long time (if not forever). One approach is to simply start working, and then iteratively try to produce better and better work. . For sure, that is one way to go, and it may work… eventually. . A better approach, I believe, is to first create a mental map for yourself—a characterization of what good work actually looks like. Then, once you have a clear picture of your target, you should be able to achieve your goal more quickly. You will make more consistent progress if you can recognize your goal and keep it in sight; you will better able to recognize when you are going astray, thereby wasting precious time; and you will be better able to know when you actually produced good work and should take a well-earned break to celebrate. . Of course, it helps if I can be specific. Your work—your research—involves building statistical models to describe, predict, and hopefully influence reality. Without further ado, here is my best attempt at painting a picture for you of what good research and good models look like. . Good research . Let’s start broad, with research as a whole, and then we’ll narrow our focus to modeling later. . At the highest level, I think good research should do things. In fact, I believe good research should do many things, all at once. And as we both know, multi-tasking is hard! In this case, I think it’s hard, but worthwhile… . Here then are the things I think good research should do, along with links to the many people/places that seeded these ideas in the first place. . Overall, I believe good research should . expose one’s ideas for the express purpose of scrutiny and criticism. This is how truth and progress are discovered and how society inches closer to them. | demonstrate why and how past ideas were wrong or incomplete. | answer a fundamental / important question (i.e., rule 8). | stand on the shoulders of giants (i.e., be thoroughly rooted in prior work). | tailor its methods to the questions being asked (i.e., qualitative and quantitative methods, in their many varieties, all have their uses and time to shine). | be based on adequate data for the question at hand. Note I mean adequate both in size and quality (i.e., measurement is important). | be rigorous. All techniques used should be thoroughly interrogated for appropriateness / external validity / internal validity. | . In additional to the points above, if we go further to think of methodological research (in the sense of statistical methodology), then I think one’s research should: . report substantively significant, not just statistically significant, results. | provide artifacts (e.g., software, procedures) that are practically useful outside of academia. | paint a “thick” picture of a method: Why does the method work? | When should the method not be expected to work well? | When should the method be expected to work well? | How robust is the method when its justifying conditions are not met? | What indicators tell one that the method is working? | What indicators tell one that the method is not working? | . | understand when to use descriptive versus predictive versus causal inference methods. (And also understand when such distinctions are blurred or unnecessary.) | be (even more) rigourous, i.e., make use of: code/software tests (e.g., unit tests) | numeric tests of the validity of one’s methods (e.g., goodness-of-fit tests) | graphical tests of the validity of one’s methods (e.g., posterior predictive checks) | . | recognize uncertainty as a first-class citizen. At the end of the day, quantifying and reducing uncertainty is the main point of one’s statistical methods. | . Good models . Alright, I understand you might be feeling overwhelmed (“unit tests; posterior predictive checks—what are those?”), and you may want to go back to whatever you were doing before you started dreaming about your future self. But before you do, let’s talk about good models. Given the extent to which you will find yourself building models in the future, I think this topic demands an entire section to itself. . On the whole, I think good models should: . be reproducible and replicable, (i.e., release the source code and data (or a representative dataset if privacy concerns are an issue)). | be thoroughly checked / falsified / investigated before estimation (e.g., prior predictive checks) | during estimation (e.g., training monitoring and diagnostics) | after estimation (e.g., posterior predictive checks, sanity checks, cross-validation, etc.) | . | provide a great description of reality / the data it is based on (i.e., pass the many checks described above). | be the result of an iterative process of model-building and model-criticism (i.e., Box’s Loop) | be only as complicated as necessary to adequately describe reality. What is adequate is of course up for debate (and debate you will). | be tailored to the problem at hand. Cookbook applications of existing methods will rarely be completely satisfying. | explicitly represent the treatment assignment mechanism in addition to the outcome, if one’s goal is to make causal inferences from the model. | be well understood. We should understand why and when our models work and fail. | be extensively visualized. We should never trust any summary statistics on their own, and this includes parameter estimates. Statistics may lie, but looking at the data and models reveals many truths. | . How to proceed . Together, all of the bullet-points above may seem like a lot. It is. . You will likely not meet all of these criteria in all of your research projects. In fact, none of your model building efforts from your PhD will meet all of these criteria. That is fine! Remember, we all start somewhere. . The point is not to beat yourself up about it, but to learn as quickly as possible and always do better the next time. . Of course, while following this path, you will occasionally find yourself in disagreement with others over how to produce good work. At those times, you should first try to learn from the other viewpoint. Extract all the good that you can, and do not presume that you know best or that the other viewpoint is wrong. Second, once you have learned all you can, if the opposing viewpoints still appear problematic, you should try improving the research aesthetics of the environment within which you are operating. Try hard, as most people around you will also want to do great work. . However, if you are unsuccessful in creating an environment that is conducive to you doing work that you believe is good, you should leave the situation as quickly as possible! In places where your research aesthetics do not align with one’s larger group or organization, you will not be appreciated to the extent you desire. Your internal reputation will run the risk of suffering due to misalignment, and your external reputation will be at risk if you are associated with work that does not fit the description of good to which you ascribe. As in the 2017 film by Jordan Peele, Get Out! . In all seriousness though, heed the words of Jan Fields (2011) to . Remember that your reputation is everything. You build your personal brand through everything you do, whether big actions or small decisions, and that brand will stay with you throughout your career. . Build a career that you can be proud of by always doing the best work you can. .",
            "url": "https://timothyb0912.github.io/blog/philosophy/2020/04/20/Good-work.html",
            "relUrl": "/philosophy/2020/04/20/Good-work.html",
            "date": " • Apr 20, 2020"
        }
        
    
  
    
        ,"post24": {
            "title": "On the need for structure... directory structure",
            "content": "On the need for structure… directory structure . Dear Young Tim, . It’s March again… In 2014, you will begin conducting your own, original research. This work will be an ongoing effort. First, it will become your Master’s thesis, and then, part of your doctoral dissertation. . And so, you will start with gusto. You’ll start with much good intent for cyclists all throughout the United States. And you’ll start without a single shred of good practice or organization. . By March 2016, you will have learned the hard way. You will have spent hundreds of hours tracking down lost or moved files, debugging cryptic errors that never appeared in 2014, and struggling to recall / recreate how your code was designed in the first place. . I’m sorry I couldn’t spare you in advance in my time. But, hopefully, I can be of more assistance now. So, without further ado, here’s what I think you did not understand then about the need for structure in one’s project directory, and here’s how I think you can do better now. . Why lacking structure hurts . You . The first reason why you should have a clear directory structure is that without one, you will suffer. Most acutely, you will suffer when you need to recover old work at the request of others or yourself but you cannot do so easily. Grant projects and research opportunities that otherwise should have been pleasant experiences will be painful toils as you exert huge amounts of energy to recreate old, unorganized projects. Indeed… you will feel real sadness when you wish to extend your own work but you cannot do so. . The second reason why you should have a clear directory structure is that you will feel the sting of a thousand little cuts without it. Not only will you waste time as you retrospectively try to recreate projects, you will waste time in new projects as you reinvent old wheels instead of easily reusing previous work. E.g., how do we need to package our tex files for uploading to arXiv again? Eventually, you will add insult to injury as you realize how silly you were to end up in this situation in the first place. . Society . Beyond yourself, there are at least a couple of additional reasons to use a clear directory structure for each research project. . First, there is no easy way to “stand on the shoulders of giants” without a well structured project, so all of society suffers. Each person who wishes to use your work will face an unnecessarily hard task of learning how to interact with one’s work (i.e. how it’s structured) in order to use the raw materials that led to your research discoveries. This is a collective waste of society’s time as each person attempts to figure out the same thing (possibly incorrectly). . Secondly, without a clear project structure, it becomes more difficult for society to trust your work. If someone cannot easily see how your project works, then they are less likely to be convinced that your project works. It is then both a waste of your time as a researcher and their time as a research consumer to have to perform additional efforts to validate the project’s results because one’s work was not immediately understandable. . How to structure your directories . Here, then, are three guiding principles for how to structure one’s project directory to avoid the aforementioned burdens on yourself and society. . Use a template . Use a template for your project directories. . Templates have at least three reasons supporting their use: . They remove much guesswork around setting up a clear project structure. | It is far easier to be a critic than a creator. You will be more likely to edit a decent project structure than to create a good project structure from nothing. | Humans are lazy (see reason two). Young Tim, it is likely that you will stick with most defaults of what you begin with. Accordingly, it’s better to begin with something good rather than begin with something bad. I.e. begin a good template instead of a monolith empty directory, or worse, a directory mixed with pre-existing work. | Keep an open mind . While its good to use a template, it is even better to modify the template as needed to meet one’s project needs. . The basic idea is simple. Be not defined by the structure of the template, but define a structure based on the template. . Have the big picture in mind . When defining a structure for one’s project, keep in mind all aspects of the project. . For instance, your research projects will almost always consist of four parts. You will likely . write code to do something (pl.create_choice_model(...)) | produce some analytic results based on data (e.g. charts, tables, statistics, etc.) | write documents of various kinds (research article, blog post, project documentation, etc.) | write more code to do things (e.g., pdflatex doc.tex; bibtex doc.aux; pdflatex doc.tex; pdflatex doc.tex) | . The structure of your project directory should encompass all the parts above so that you keep the project contained in one location. . What good structure brings . Once you adopt the project structuring philosophy above, you will begin to experience subtle-yet-tangible benefits to your life. Here are three such benefits. . One: you will become a magician. Having a template for a good project structure minimizes your recurring research efforts. And this transfer of a variable cost to a fixed cost is nothing short of magical! It’s these sort of changes that will drive major productivity gains in your life. . Two: you will become a more social reseacher. I know. It’s hard to believe me since you’re already so social… Hear me out. You have a sense of pride. We both know it. When your project structure is a mess, you will not want to share your work with anyone. But when you’ve got a good project structure, you’ll proudly share it with everyone and their families. And we already know that sharing your work is good for you and good for everyone else too. . Three: you will be proud of yourself. As mundane as it may be, your project structure is another one of your creations. It’s an object whose design you can take pride in. All your life you’ve admired people whose creations were deliberate, useful, awesome, and full of humor / good-cheer. Use your project structure to display the same traits, =]. . Practical resources . Okay, okay. By now, you’re sold on the general idea. Let’s talk specifics then. . You’ll typically work on projects that fall into one or more of the following three categories. . There will be “analysis” projects where the primary aim is to analyze one or more datasets or demonstrate some method of analysis. | There will be software projects where the primary aim is to produce reusable tools in code for yourself and others. | And, there will be writing projects where the primary aim is to produce some written document, with or without supporting data analysis and code. | Corresponding to each of these categories there are (or one should make) reusable templates that set up a good project structure for the task at hand. For analysis projects, I recommend templates based on Cookiecutter / Cookiecutter-DataScience tools. For the PyLogit’s in your future, I recommend templates based on Pyscaffold / Pyscaffoldext-dsproject tools. For your writing projects, however, I have a wider range of recommendations… . For blogs, I recommend templates based on tools from the future like fast_template. (And yes you should blog during graduate school…) For books, I recommend templates based on Jupyter-Book. For articles, I recommend you make your own =). For some inspiration, see resources such as the following. . That’s all for now. I have more recommendations (for instance, basing PyTorch projects on the PyTorch-Template-Project), but I’ll leave you to explore. Go forth, and set up projects with ease and care! . You and everyone one else will thank you in the future. .",
            "url": "https://timothyb0912.github.io/blog/practice/2020/03/14/P1-cookiecutter-intro.html",
            "relUrl": "/practice/2020/03/14/P1-cookiecutter-intro.html",
            "date": " • Mar 14, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "Hi there, my name is Timothy Brathwaite. . I’m a data scientist. Previously, I worked a data scientist at Stitch Fix and as a research scientist at Lyft. . Before this, I completed my PhD (2018) at UC Berkeley in Transportation Engineering, under the advising of Professor Joan L. Walker. . My research, then and now, focuses on discrete choice modeling, machine learning, applied statistics, and causal inference. In terms of applications, bicycle demand modeling is a topic near and dear to my heart. So much so that I focused my entire dissertation on it. . In this blog, you’ll find my thoughts, reflections, and short snippets of code demonstrating my ideas on the topics above. . Feel free to have a look around, and thanks for your interest! . This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://timothyb0912.github.io/blog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  

  
  

  
      ,"page11": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://timothyb0912.github.io/blog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}